---
layout: post
title:  "Blackwell NVFP4 Kernel Hackathon Part 2: GEMM kernel"
date:   2025-12-22 23:18:18 +0000
tags: [gemm, nvfp4, cutedsl, B200]
---

# Introduction
## Contest Recap
[Blackwell NVFP4 Kernel Hackathon](https://luma.com/9n27uem4?tk=EfycNU) welcomes hackers to develop the fastest kernels for [NVFP4](https://docs.nvidia.com/cutlass/media/docs/cpp/fundamental_types.html) format, a low precision micro-block scaling value format, on Blackwell B200. It involves 4 kernels: GEMV ([leaderboard](https://www.gpumode.com/v2/leaderboard/595?tab=rankings)), GEMM ([leaderboard](https://www.gpumode.com/v2/leaderboard/597?tab=rankings)), Gated Dual GEMM, and Grouped GEMM.

This blogpost is on the second kernel: GEMM, finished on 12/20/2025. It mainly focuses on how to do gemm with scaling factors, and simplifications based on the [example kernel implementation](https://github.com/NVIDIA/cutlass/blob/main/examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py). Using the simplification techniques in the blogpost helped me achieved no 6 in the competition among 100+ participants.

If you want to learn how to build a fast GEMM using `tcgne05.mma`, nothing tops the **GOATED** [blogpost by gau.nerst](https://gau-nernst.github.io/tcgen05) which has thorough details and nice visual illustrations.

## Problem Sizes
The GEMM problem sizes and their speed-of-light latency:

| M | N | K | L | time\[Âµs] |
| - | - | - | - | - |
| 128 | 7168 | 16384 | 1| 8.994 |
| 128 | 4096 | 7168 | 1 | 2.354 |
| 128 | 7168 | 2048 | 1 | 1.333 |

# tcgen05 for NVFP4
Block scaling GEMM problem formula: `(A * scale_A)  * (B * scale_B) + D`

## Overall
The PTX section on tcgen05 for block scaling formats: https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-block-scaling

Data flow
- A/B matrics: GMEM -> SMEM (optionally A can be copied to TMEM)
- SFA/SFB matrics: GMEM -> SMEM -> TMEM
- C/D matrics: TMEM -> register -> SMEM (optional) -> GMEM

Hardware for data movement:
- GMEM -> SMEM: TMA
- SMEM -> TMEM: [tcgen05.cp](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-cp)
    - PTX: `tcgen05.cp.cta_group::1.32x128b.warpx4`
    - CuteDSL: `tcgen05.Cp4x32x128bOp(tcgen05.CtaGroup.ONE)`
- SMEM -> GMEM: TMA
- TMEM -> register: [tcgen05.ld](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-ld)
    - PTX: `tcgen05.ld.sync.aligned.32x32b.x128.b32`
    - CuteDSL: `tcgen05.Ld32x32bOp(tcgen05.Repetition.x128, tcgen05.Pack.NONE)`
- TMEM -> SMEM: not supported
- Register -> SMEM: sync copy, sync threads
- Register -> GMEM: sync copy
    - CuteDSL: `cute.nvgpu.CopyUniversalOp()`

Example NVFP4 PTX qualifiers:
```
tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.scale_vec::4X.collector_usage
                                [d-tmem],  a-desc,  b-desc, idesc,
                                [scale-A-tmem], [scale-B-tmem], enable-input-d;
```

{% include nvfp4-gemm/tcgen05-mma-scale-vec-2x.png %}

The diagram is for other block scaling formats because it only has 2x; NVFP4 requres scale vector 4X.

## Supported Tile Sizes

{% include nvfp4-gemm/supported-tile-sizes.png %}
[Source: PTX doc on tcgen05 kind shapes](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-kind-shapes)

The table means, for NVFP4
- For 1 CTA group: it supports 128xNxK, where N = {8, 16, ...256} with steps of 8, and K = 64
- For 2 CTA groups: it supports 256xNxK, where N = {16, 32, ...256} with steps of 16, and K = 64

Additionally, although the table shows what sparse matrics shapes are supported, PTX doc mentions [here](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-sparse-matrices) that NVFP4 doesn't support sparse matrics, so K has to be 64. This restriction is implemented in [sm100_utils.make_blockscaled_trivial_tiled_mma()](https://github.com/NVIDIA/cutlass/blob/b7ecaa605dd70326900433695e11ebfec407edd2/python/CuTeDSL/cutlass/utils/blackwell_helpers.py#L952)

## SFB TMEM Layout
There aren't a lot of choices for M tile sizes, so the focus would be on tuning the N tile sizes. We need to study the layout of SFB, more specifically, SFB layout on TMEM. tcgen05 has a very specific layout requirement for NVFP4 SFB as documented here: https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-factor-b-layout-4x

The figure is the layout of scale factor B matrix with scale_vec::4X/block16 with K=64/K=128, exactly what we need. Unfortunately, I stared at the diagram many times without understanding what it means. One reason is that it only shows 32 lanes while the TMEM has 128 lanes.

I was able to figure it out after many hours so you won't need to spend so much time on it :)

{% include nvfp4-gemm/b-vs-sfb-layout.png %}

First, for a B matrix tile, it's shape N tile x K tile is 128 x (64 x 4). Using N tile 128 as an example which is the same as [num TMEM lanes](https://docs.nvidia.com/cuda/parallel-thread-execution/#tensor-memory-addressing). For an SBF tile, it's shape is 128 x (4 x 4) because NVFP4 block size is 16.

{% include nvfp4-gemm/sfb-layout.png %}

This diagram just zoom in on the indices: there are N0 to N127 rows, and SF0-SF3, SF4-SF7, SF8-SF11, SF12-SF15 columns for the 4 sub tiles.

{% include nvfp4-gemm/sfb-tmem-layout.png %}

This diagram shows the magic. It chops each matrix along Nx % 32 == 0 rows, and put the same sub tiles to the same 32 rows. Expanding it to N == 255 will give us the same diagram as in https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-factor-b-layout-4x.

This explains some details in the [example kernel](https://github.com/NVIDIA/cutlass/blob/main/examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py):
- Why it requires 16 TMEM columns for SFB: because 128 / 32 * 4 = 16
- How does the [n tile = 64 offset](https://github.com/NVIDIA/cutlass/blob/b7ecaa605dd70326900433695e11ebfec407edd2/examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py#L1193) work: it needs to offset 64 / 32 * 4 = 8 columns. Using acc_ptr as the base ptr, since it's 32 bit vs 8 bit, it needs to offset 8 / (32 / 8) = 2 columns.

# Simplifications
## Base Kernel
The [base kernel](https://github.com/NVIDIA/cutlass/blob/main/examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py) is already super optimized. It implements the advanced Blackwell warp specializations with persistent scheduling.

{% include nvfp4-gemm/blackwell-ws.png %}

The additional epilogue warp is made possible by TMEM.

{% include nvfp4-gemm/tmem-warp-mapping.png %}

For the small problems like those in the contest, reducing the latency of a already well tuned kernel requires reducing the kernel prologue overhead and simplifying epilogue.

## Tune Tiling Sizes
The default N tile size is 128:
- For problem 1 and 3, num M tile: 1, num n tiles: 7168 / 128 => 56 SMs
- For problem 2, num M tile: 1, num n tiles: 4096 / 128 => 32 SMs

All problems significantly under utilizes B200 SMs. A simple optimization is changing the N tile size to be 64, then
- For problem 1 and 3, num M tile: 1, num n tiles: 7168 / 64 => 112 SMs
- For problem 2, num M tile: 1, num n tiles: 4096 / 64 => 64 SMs


```py
# SFB N tile size is always at least 128 because tcgen05.ld only supports 128.
self.mma_inst_shape_mn_sfb = (
    self.mma_inst_shape_mn[0] // (2 if self.use_2cta_instrs else 1),
    cute.round_up(self.mma_inst_shape_mn[1], 128),
)

# Find the correct GMEM slice for SFB as SFB
if cutlass.const_expr(self.cta_tile_shape_mnk[1] == 64):
    slice_n = mma_tile_coord_mnl[1] // 2
tBgSFB_slice = tBgSFB[
    (None, slice_n, None, mma_tile_coord_mnl[2])
]

# Offset TMEM Columns
elif cutlass.const_expr(self.cta_tile_shape_mnk[1] == 64):
    # Move in increments of 64 columns of SFB
    offset = cutlass.Int32((mma_tile_coord_mnl[1] % 2) * 2)
    shifted_ptr = cute.recast_ptr(
        acc_tmem_ptr
        + self.num_accumulator_tmem_cols
        + self.num_sfa_tmem_cols
        + offset,
        dtype=self.sf_dtype,
    )
```

## Remove Persistent Scheduling
Based on the problem sizes, we could observe that each only requires 1 wave to compute. Persistent scheduling helps reduce the kernel prologue when there are many waves. However, scheduling isn't needed at all for this problem that only requires 1 wave. We could directly use `bidx, bidy, bidz` to locate the tiles.

## Pick up the latest updates
A bunch of small updates with the release of cutlass 4.3:

1. Replace
```py
# Cluster arrive after barrier init
if cute.size(self.cluster_shape_mn) > 1:
    cute.arch.cluster_arrive_relaxed()
```

With
```py
pipeline_init_arrive(cluster_shape_mn=self.cluster_shape_mn, is_relaxed=True)
```

2. Replace
```py
#
# Cluster wait before tensor memory alloc
#
if cute.size(self.cluster_shape_mn) > 1:
    cute.arch.cluster_wait()
else:
    self.cta_sync_barrier.arrive_and_wait()
```

With
```py
pipeline_init_wait(cluster_shape_mn=self.cluster_shape_mn)
```

3. Add `defer_sync=True,` to pipeline create.

## Remove some Epilogue Syncs
Removed these syncs in epilogue as they are not needed in 1 wave compute.

```py
# Fence and barrier to make sure shared memory store is visible to TMA store
cute.arch.fence_proxy(
    cute.arch.ProxyKind.async_shared,
    space=cute.arch.SharedSpace.shared_cta,
)

# After compute
self.epilog_sync_barrier.arrive_and_wait()
```

# Further Optimizations
## Multicast
Since multiple SMs share the same A matrix, we could leverage the TMA multicast capability to enforce loading data from L2 cache when possible.

## Prefetch Tiles
The current implementation makes TMA warp and MMA warp work in tendom. Instead we could issue a bunch of TMA instructions without waiting / signally MMA at the beginning, and then assume the regular TMA main loop.

## Split K
Split K could further increase the SM utilization especially with persistent scheduling.

# Optimizations that Didn't Help Or Doesn't Work
## Directly store from register to gmem
The original implementation writes accumulation results to TMEM -> register -> SMEM -> GMEM. One would naturally wonder what happens if we remove the additional step of storing to SMEM, modify the data flow to be TMEM -> register -> GMEM. Thanks [Simon](https://www.linkedin.com/in/simon-veitner-174a681b6/) for helping with the implementation.

Unfortunately it made the kernel slower. I guess (haven't confirmed) reasons could be the
- TMA is async
- TMA can help achieve higher memory bandwidth

## Use N Tile Size 32
As calculated above, problem 2 only uses 64 SMs. It uses < 50% of B200 SMs, so natually we want to divide the matrices into smaller tiles. 1 simple way is using smaller N tile size. N tile size 32 should use 132 SMs. From a section above, [the PTX doc](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-kind-shapes) says N tile size 32 is supported.

However, the kernel throws error when I try to offset the SFB TMEM column by 4. Confirmed with Nvidia folks that there's restriction on the TMEM alignment. It's not in the official document. A workaround is loading SFB from SMEM to register, then offset the registers when copying to TMEM.

## Use only 1 Warp for Epilogue
C is stored in TMEM. 4 epilogue warps access the accumulate results, copy to SMEM then finally GMEM. 1 way to reduce the epilogue latency is removing sync needed across the warps after copying to SMEM. However this is not possible due to the [TMEM access restriction](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-tensor-memory-ld-st-access-restrictions): "each warp of a warpgroup in the CTA can access **a chunk** of the Tensor Memory". More specifically, warp 0 can access lay 0-31, it can't access lanes 32-127. That means if we want to reduce the num epilogue warps to 1, it requires changing the accumulated result layout, which isn't possible.

{% include nvfp4-gemm/tmem-warp-mapping.png %}

# Result
I was fortunate to rank 6 (shiyegao and shigao are the same person) among 100+ participants. This far exceeded my original expectation as I only stomped into the GEMM field for 4 months. Many thanks to [Simon](https://www.linkedin.com/in/simon-veitner-174a681b6/) and [Yue](https://www.linkedin.com/in/yue-zhang-ishere/) for discussions and helping.

# Resources
- https://veitner.bearblog.dev/blog/
- https://gau-nernst.github.io/tcgen05
- https://github.com/zartbot/blog/issues/3
- https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-1-introduction
- https://github.com/Dao-AILab/quack
- https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/cute/flash_fwd_sm100.py