---
layout: post
title:  "DeepSeek DeepGEMM Study Note Part 1: The Kernel"
date:   2024-03-08 18:18:18 +0000
tags: gemm,fp8
---

### **Series Overview: Deciphering DeepGEMM and FP8 GEMM**


DeepSeek recently open-sourced [a series of infrastructure repositories](https://github.com/deepseek-ai/open-infra-index), with one of the core projects being [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM). As described in its own words:

> "DeepGEMM is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine-grained scaling, as proposed in [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3)."

At its core, matrix multiplication is fundamental to modern machine learning, powering everything from training neural networks to running real-time inference on images and text. FP8 GEMM represents a significant leap forward-leveraging 8-bit floating-point precision to accelerate these computations while maintaining accuracy.

In this series of posts, we'll dive into the DeepGEMM source code to explore how it achieves high-performance FP8 GEMM.

Here we go!

---

### **Background: What is GEMM?**
GEMM (General Matrix Multiply) is the foundational operation for multiplying two matrices ($$ A $$ and $$ B $$) and adding their product to a third matrix $$ C = \alpha AB + \beta C $$

where $$ \alpha $$ and $$ \beta $$ are scalars. This simple equation powers tasks like forward/backward propagation in neural networks, making GEMM one of the most compute-intensive operations in AI frameworks (e.g., PyTorch, TensorFlow).

There are many great resources on how to accelerate GEMM
- [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM)
- [CUTLASS Tutorial: Efficient GEMM kernel designs with Pipelining - Colfax Research](https://research.colfax-intl.com/cutlass-tutorial-design-of-a-gemm-kernel/)
- [cutlass/media/docs/efficient_gemm.md at main](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md)

Traditionally, these calculations use 32-bit floating-point numbers (FP32) for precision but suffer from high memory and bandwidth demands. As models grow larger and efficiency becomes paramount, reducing computational cost while maintaining accuracy is crucial. This is where FP8 (8-bit floating point) General Matrix Multiplication (GEMM) comes into play.

---

### **Why FP8 GEMM Matters**

#### **Lower Precision, Higher Efficiency**
Traditional deep learning training and inference rely on FP32 or FP16 precision, which offers a balance between accuracy and computational efficiency. However, FP8 reduces the bit-width even further, enabling **higher throughput** with faster computation and lower memory bandwidth consumption. This allows AI accelerators and GPUs to execute more operations per second, leading to faster training and inference times. This is one of the secrete sources behind why [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) is so cost efficient in training and inferencing.

#### **Energy Savings and Hardware Utilization**
Energy efficiency is a growing concern in AI model scaling. Lower-bit computations, such as FP8, reduce power consumption significantly compared to FP16 or FP32. This makes FP8 GEMM an attractive option for cloud providers and enterprises focused on reducing the carbon footprint of large-scale AI workloads.

#### **Maintaining Accuracy with Optimized Algorithms**
One of the primary challenges of using lower precision is maintaining numerical stability and model accuracy. FP8 formats, particularly with dynamic scaling techniques and mixed-precision training, have been designed to mitigate precision loss. Many modern AI frameworks and hardware accelerators now support FP8 computation with enhanced quantization techniques, ensuring minimal degradation in model performance.

#### **Industry Adoption and Future Trends**
Leading AI hardware vendors, such as NVIDIA, Google, and AMD, are actively incorporating FP8 into their latest accelerators. The adoption of FP8 GEMM in frameworks like PyTorch and TensorFlow further solidifies its importance in the AI landscape. As models continue to scale, FP8 is poised to become the standard for deep learning efficiency.


---

### **How Does FP8 GEMM Work?**
FP8 is a compact floating-point format with limited precision but optimized for AI workloads. There are two common variants:

#### **1. Data Representation Formats**
   - **E4M3 (Exponent 4, Mantissa 3):**
     - 1 sign bit, 4 exponent bits (range $$ \pm 2^{-8} $$ to $$ 2^7 $$), and 3 mantissa bits (no implicit leading 1).
     - Better for activations due to wider dynamic range.
   - **E5M2 (Exponent 5, Mantissa 2):**
     - 1 sign bit, 5 exponent bits (-15 to +16), and 2 mantissa bits.
     - More precise for weights/gradients but narrower range.

For more details on data types, checkout out my [other post]({% post_url 2024-02-04-numeric-data-types-1 %})

#### **1. Key Steps in FP8 GEMM**
   - **Step 1: Quantization**
     Convert inputs (FP32/FP16) into FP8 using scaling factors to minimize precision loss. For example, scale values by a scalar $$ s $$ such that $$ x_{\text{FP8}} = \text{round}(x/s) $$.
   - **Step 2: Matrix Multiplication**
     Multiply matrices $$ A_{\text{FP8}} $$ and $$ B_{\text{FP8}} $$. Hopper GPUs natively support [8-bit MMA](https://docs.nvidia.com/cuda/parallel-thread-execution/#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n32) (matrix multiply and add).
   - **Step 3: Accumulation in Higher Precision:**
     Sum partial products using FP16 or FP32 to prevent excessive precision loss.
   - **Step 4: Dequantization**
     Convert the result back to a higher precision format (FP16/FP32) using inverse scaling (\(y = y_{\text{FP8}} \times s'\)), ensuring compatibility with downstream operations.

<img src="{{site.baseurl}}/assets/deepgemm/New-Hopper-FP8-Precisions.jpg">

Image source: [NVIDIA Hopper Architecture In-Depth \| NVIDIA Technical Blog](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/)

#### **2. Challenges & Mitigations**
   - **Overflow/Underflow:** FP8's limited exponent range risks losing extreme values. Solutions include dynamic quantization (adjusting scales per layer/batch) or mixed-precision strategies.
   - **Accuracy Loss:** Quantization-aware training (QAT) adjusts models to tolerate lower precision, often with minimal accuracy drops.

---

### **DeepGEMM Architecture**
DeepGEMM adopts [Ping-Pong architecture](https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel)

> Ping-Pong is one of the fastest matmul (GEMM) kernel architectures available for the Hopper GPU architecture.

<img src="{{site.baseurl}}/assets/deepgemm/CUTLASS-Ping-Pong-GEMM-Kernel.png">

While it didn't create a new architecture, it incorporates many techniques to accelerate:
- Fine-grained control: tuned [PTX](https://docs.nvidia.com/cuda/parallel-thread-execution) instructions, registers count, and overlapping copy/compute.
- Full JIT design: compile at runtime to tune the best possible config.
- Support unaligned block sizes: fully utilize SMs for non-typical block sizes.
- FFMA SASS interleaving: modify binary code to explicitly instruct warp scheduler.

---

### **What's Next**
We will dive into key optimizing techniques to make DeepGEMM fast in the next posts. Stay tuned!
