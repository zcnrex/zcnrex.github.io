---
layout: post
title:  "DeepSeek DeepGEMM Study Note Part 1: The Kernel"
date:   2024-03-08 18:18:18 +0000
tags: gemm,fp8
---

### **Series Overview: Deciphering DeepGEMM and FP8 GEMM**


DeepSeek recently open-sourced [a series of infrastructure repositories](https://github.com/deepseek-ai/open-infra-index), with one of the core projects being [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM). As described in its own words:

> "DeepGEMM is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine-grained scaling, as proposed in [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3)."

At its core, matrix multiplication is fundamental to modern machine learning, powering everything from training neural networks to running real-time inference on images and text. FP8 GEMM represents a significant leap forward-leveraging 8-bit floating-point precision to accelerate these computations while maintaining accuracy.

In this series of posts, we'll dive into the DeepGEMM source code to explore how it achieves high-performance FP8 GEMM.

Here we go!

---

### **Background: What is GEMM?**

GEMM (General Matrix Multiply) is the foundational operation for multiplying two matrices ($$ A $$ and $$ B $$) and adding their product to a third matrix $$ C = \alpha AB + \beta C $$

where $$ \alpha $$ and $$ \beta $$ are scalars. This simple equation powers tasks like forward/backward propagation in neural networks, making GEMM one of the most compute-intensive operations in AI frameworks (e.g., PyTorch, TensorFlow).

There are many great resources on how to accelerate GEMM
- [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM)
- [CUTLASS Tutorial: Efficient GEMM kernel designs with Pipelining - Colfax Research](https://research.colfax-intl.com/cutlass-tutorial-design-of-a-gemm-kernel/)
- [cutlass/media/docs/efficient_gemm.md at main](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md)

Traditionally, these calculations use 32-bit floating-point numbers (FP32) for precision but suffer from high memory and bandwidth demands. As models grow larger and efficiency becomes paramount, reducing computational cost while maintaining accuracy is crucial. This is where FP8 (8-bit floating point) General Matrix Multiplication (GEMM) comes into play.

---

### **Why FP8 GEMM Matters**

#### **Lower Precision, Higher Efficiency**
Traditional deep learning training and inference rely on FP32 or FP16 precision, which offers a balance between accuracy and computational efficiency. However, FP8 reduces the bit-width even further, enabling **higher throughput** with faster computation and lower memory bandwidth consumption. This allows AI accelerators and GPUs to execute more operations per second, leading to faster training and inference times. This is one of the secrete sources behind why [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) is so cost efficient in training and inferencing.

#### **Energy Savings and Hardware Utilization**
Energy efficiency is a growing concern in AI model scaling. Lower-bit computations, such as FP8, reduce power consumption significantly compared to FP16 or FP32. This makes FP8 GEMM an attractive option for cloud providers and enterprises focused on reducing the carbon footprint of large-scale AI workloads.

#### **Maintaining Accuracy with Optimized Algorithms**
One of the primary challenges of using lower precision is maintaining numerical stability and model accuracy. FP8 formats, particularly with dynamic scaling techniques and mixed-precision training, have been designed to mitigate precision loss. Many modern AI frameworks and hardware accelerators now support FP8 computation with enhanced quantization techniques, ensuring minimal degradation in model performance.

#### **Industry Adoption and Future Trends**
Leading AI hardware vendors, such as NVIDIA, Google, and AMD, are actively incorporating FP8 into their latest accelerators. The adoption of FP8 GEMM in frameworks like PyTorch and TensorFlow further solidifies its importance in the AI landscape. As models continue to scale, FP8 is poised to become the standard for deep learning efficiency.


---

### **How Does FP8 GEMM Work?**
FP8 is a compact floating-point format with limited precision but optimized for AI workloads. There are two common variants:

#### **1. Data Representation Formats**
   - **E4M3 (Exponent 4, Mantissa 3):**
     - 1 sign bit, 4 exponent bits (range $$ \pm 2^{-8} $$ to $$ 2^7 $$), and 3 mantissa bits (no implicit leading 1).
     - Better for activations due to wider dynamic range.
   - **E5M2 (Exponent 5, Mantissa 2):**
     - 1 sign bit, 5 exponent bits (-15 to +16), and 2 mantissa bits.
     - More precise for weights/gradients but narrower range.

For more details on data types, checkout out my [other post]({% post_url 2024-02-04-numeric-data-types-1 %})

#### **1. Key Steps in FP8 GEMM**
   - **Step 1: Quantization**
     Convert inputs (FP32/FP16) into FP8 using scaling factors to minimize precision loss. For example, scale values by a scalar $$ s $$ such that $$ x_{\text{FP8}} = \text{round}(x/s) $$.
   - **Step 2: Matrix Multiplication**
     Multiply matrices $$ A_{\text{FP8}} $$ and $$ B_{\text{FP8}} $$. Hopper GPUs natively support [8-bit MMA](https://docs.nvidia.com/cuda/parallel-thread-execution/#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n32) (matrix multiply and add).
   - **Step 3: Accumulation in Higher Precision:**
     Sum partial products using FP16 or FP32 to prevent excessive precision loss.
   - **Step 4: Dequantization**
     Convert the result back to a higher precision format (FP16/FP32) using inverse scaling (\(y = y_{\text{FP8}} \times s'\)), ensuring compatibility with downstream operations.

<div style="text-align: center;">
<img src="{{site.baseurl}}/assets/deepgemm/New-Hopper-FP8-Precisions.jpg">

Image source: <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">NVIDIA Hopper Architecture In-Depth | NVIDIA Technical Blog</a>
</div>

#### **2. Challenges & Mitigations**
   - **Overflow/Underflow:** FP8's limited exponent range risks losing extreme values. Solutions include dynamic quantization (adjusting scales per layer/batch) or mixed-precision strategies.
   - **Accuracy Loss:** Quantization-aware training (QAT) adjusts models to tolerate lower precision, often with minimal accuracy drops.

---

### **DeepGEMM Architecture**
DeepGEMM adopts [Ping-Pong architecture](https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel)

> Ping-Pong is one of the fastest matmul (GEMM) kernel architectures available for the Hopper GPU architecture.

<img src="{{site.baseurl}}/assets/deepgemm/CUTLASS-Ping-Pong-GEMM-Kernel.png">

While it didn't create a new architecture, it incorporates many techniques to accelerate:
- Fine-grained control: tuned [PTX](https://docs.nvidia.com/cuda/parallel-thread-execution) instructions, registers count, and overlapping copy/compute.
- Full JIT design: compile at runtime to tune the best possible config.
- Support unaligned block sizes: fully utilize SMs for non-typical block sizes.
- FFMA SASS interleaving: modify binary code to explicitly instruct warp scheduler.

---

### Overall Code Structure
The [kernel fp8_gemm.cuh](https://github.com/deepseek-ai/DeepGEMM/blob/main/deep_gemm/include/deep_gemm/fp8_gemm.cuh) code structure

```cpp
// -------- Initialize various variables, pointers, etc --------
// TMA descriptor.
cute::prefetch_tma_descriptor(...)

// Allocate shared memory
extern __shared__ __align__(1024) uint8_t smem_buffer[];

// TMA Barrier for both divisible and non-divisible cases
Barrier* full_barriers[kNumStages];
Barrier* empty_barriers[kNumStages];

// For pipeline unrolling
auto launch_k_iterations = [](const auto& func)

// Block scheduler
auto scheduler = Scheduler<>()

// -------- Implementation of the above diagram --------
if (threadIdx.x >= kNumMathThreads) {
    // TMA warp-group for loading data
} else {
    // Math warp-groups for WGMMA
}
```

It reflect the ping-pong architecture
<img src="{{site.baseurl}}/assets/deepgemm/deepgemm-design.png">


It uses 1 thread to handle TMA and the rest of the threads to do math works.

#### **TMA**
[**Tensor Memory Accelerator**](https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html#tensor-memory-accelerator) is a new HW unit introduced in Hopper. Itâ€™s dedicated to transfer data and supports async execution. Previously data transfer was handled by threads in computation cores.

<div style="text-align: center;">
<img src="{{site.baseurl}}/assets/deepgemm/TMA-HW.jpg">

Image source: <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">NVIDIA Hopper Architecture In-Depth | NVIDIA Technical Blog</a>
</div>

After TMA, all the computation cores (streaming processors, or SMs) can focus on the math computation while TMA handles data transfer.

Since TMA can be viewed as accessing data, the code uses file descriptor to read/write data

```cpp
// Initialization
cute::prefetch_tma_descriptor(reinterpret_cast<cute::TmaDescriptor const*>(&tensor_map_a));
```

Notice that it uses a special pointer, `tensor_map_a`. This is a handle that knows the pattern to read/write the data. It's defined in this code:

```cpp
// TMA API at kernel launch for matrix A
// "desc" is short for descriptor
static CUtensorMap make_2d_tma_a_desc(T* global_address, uint32_t shape_m) {
    return make_2d_tma_desc(global_address, Layout::RowMajor,
                            shape_m * (kGemmType == GemmType::GroupedMasked ? kNumGroups : 1), SHAPE_K, BLOCK_M, BLOCK_K);
}


static CUtensorMap make_2d_tma_desc(
        T* global_address, Layout layout,
        uint32_t gmem_rows, uint32_t gmem_cols,
        uint32_t smem_rows, uint32_t smem_cols,
        // Swizzle is introduced to reduce bank conflict when accessing matrix elements.
        CUtensorMapSwizzle swizzle_type = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B) {
    // A limitation of TMA is it can only access continuous memory, so the dims and strides are different for row major / col major accesses.
    if (layout == Layout::RowMajor) {
        uint64_t gmem_dim[2] = {gmem_cols, gmem_rows};
        uint32_t smem_dim[2] = {smem_cols, smem_rows};
        return make_2d_tma_copy_desc(global_address, gmem_dim, gmem_cols * sizeof(T), smem_dim, swizzle_type);
    } else {
        uint64_t gmem_dim[2] = {gmem_rows, gmem_cols};
        uint32_t smem_dim[2] = {smem_rows, smem_cols};
        return make_2d_tma_copy_desc(global_address, gmem_dim, gmem_rows * sizeof(T), smem_dim, swizzle_type);
    }
}

// Implementation
CUtensorMap make_2d_tma_copy_desc(T* global_address, uint64_t gmem_dim[2],
                                  uint64_t stride_in_bytes, uint32_t smem_dim[2],
                                  CUtensorMapSwizzle swizzle_type,
                                  PFN_cuTensorMapEncodeTiled encode_func = nullptr) {
    CUtensorMap tensor_map{};
    constexpr uint32_t rank = 2;
    // How to read the global memory
    uint64_t global_stride[rank - 1] = {stride_in_bytes};
    // How to write to the shared memory
    uint32_t elem_strides[rank] = {1, 1};

    if (encode_func == nullptr)
        encode_func = get_cuTensorMapEncodeTiled();

    // This is the function that fill the tensor_map with all the needed information
    auto result = encode_func(
            &tensor_map, get_CUtensorMapDataType<typename std::remove_cv<T>::type>(), rank,
            global_address, gmem_dim, global_stride, smem_dim, elem_strides,
            CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE, swizzle_type,
            CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_L2_256B,
            CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE);
    DG_HOST_ASSERT(result == CUDA_SUCCESS);
    return tensor_map;
}
```

- **TMA** can only read _continuous_ address when accessing memory
    - This raise the necessity to read `scales_a` in col major layout. Will deep dive this in a later section.
- **Swizzle** defines matrix access pattern. More infomation can be found in this [blog](https://leimao.github.io/blog/CUDA-Shared-Memory-Swizzling/#CUDA-Shared-Memory-Swizzling)
- `cuTensorMapEncodeTiled` is the core function that fills the tensor_map.

<div style="text-align: center;">
<b>Examples of Different Swizzle Layouts</b>
<img src="{{site.baseurl}}/assets/deepgemm/Cute-Layout.jpg">
</div>

#### **Why Scale A Needs To Be Col Major**
Quantization enables fast compute but low precision. To increase the precision, a scale matrix $$ A_{\text{scale}} $$ is created when quantizing matrix $$ A $$. Assume $$ A = M * N $$, then using token-group quantization, $$ A_{\text{scale}} = M * (N / group\_size) $$. Let's use some numbers for illustration:
```
Matrix A = 128 x 7168
Group size = 128
// Then Matrix A Scale is 128x smaller than Matrix A
Matrix A Scale = 128 x 56 = 128 x (7168 / 128)

// Assume we do GEMM tiling
Block size = 128 x 128

// If A scale is row major, when processing 1 block tile:
A block = 128 x 128
A scale block = 128 x 56 // This would be memory bound

// If A scale is col major, when processing 1 block tile:
A block = 128 x 128
A scale block = 128 x 1
```
The reason it needs to read the entire Matrix A Scale in row major layout is because TMA can only read continuous memory addresses.

---

### **Built-in Keywords / Concepts**
#### **Cuda Keywords**
```cpp
// cuda kernel entry point, can only be called by CPU host
__global__

// Function run on GPU
__device__

// CPU function
__host__

// shared memory
__shared__

// inline a function forcefully. This can be beneficial for performance in GPU applications, since it reduces the overhead of function calls and can lead to more optimized code execution
__forceinline__

// used to provide the compiler with information about the expected maximum number of threads per block and the minimum number of blocks per multiprocessor that will be launched for a specific kernel. This information can help the compiler optimize register and shared memory usage, potentially leading to improved performance by better utilizing the hardware resources of the GPU.
__launch_bounds__

// FP8 E4M3 numerical format
__nv_fp8_e4m3

// BF16
__nv_bfloat16

// Converts float2 (64 bits) to 2 bfloat162 (32 bits). bfloat162 = 2 bfloat16
__float22bfloat162_rn

```

#### **Cuda Concepts**
**Warp**: a group of _**32 threads**_ that execute the same instruction simultaneously, allowing fine-grained parallelism at the thread level.

**Cluster**: a higher-level grouping of warps, often used in advanced architectures for coordinating and optimizing larger-scale parallel operations. Clusters can manage multiple warps and improve overall efficiency by sharing resources and synchronizing tasks across warps.

#### **C++ Keywords**
```cpp
// prevent implicit conversions and copy-initialization that could lead to unexpected or undesired behavior. When applied to constructors, it ensures that the constructor can only be called with direct initialization and not used for implicit type conversions.
explicit

// declare a variable or a function that is defined in another translation unit (source file). This is particularly useful for sharing variables and functions across multiple files in a project.
extern

// a variable, function, or constructor can be evaluated at compile-time. This allows for more efficient and safer code, as the value or result is computed during compilation rather than at runtime.
constexpr

// allows for conversion between different pointer types, as well as between pointer types and integral types
reinterpret_cast
```