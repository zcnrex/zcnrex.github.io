# Introduction
## The contest
There is a contest https://luma.com/9n27uem4?tk=EfycNU to develop the fastest kernels for [NVFP4](https://docs.nvidia.com/cutlass/media/docs/cpp/fundamental_types.html) format, a low precision block scaled format, on Blackwell B200. It involves 4 kernels: GEMV, GEMM, Gated Dual GEMM, and Grouped GEMM.

This blogpost dives into optimization techniques of the first kernel GEMV that was conclueded on 11/29/2025. It won't include the improvement for every optimization because a different combination of the optimizations can have different effects.

## NVFP4 format
It's a micro-block scaling low precision value format. Each value block contains 16 FP4 numbers (E2M1) plus 1 shared FP8 scale (E4M3). Other micro-block scaling formats include MXFP8 and MXFP4 that have different block size, value precisions, and scaling factors.

NVFP4 achieves a good trade-off between the hardware efficiency and model performance. Here's an Nvidia [blog](https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/) that dives deep into NVFP4 efficiency and compares different block scaled low precision data formats.

## Problem sizes
Here are the problem sizes of kernel 1 and the speed of light analysis of B200:

|M  |  K | L | time\[µs] |
| - | - | - | - |
| 7168 | 16384 | 1| 8.622 |
| 4096 | 7168 | 8 | 17.275 |
| 7168 | 2048 | 4 | 4.317 |

## CuteDSL Template
[CuteDSL](https://docs.nvidia.com/cutlass/media/docs/pythonDSL/overview.html) is a python DSL for CUTLASS, a high performance CUDA kernel template library. It achieves great kernel performance with extra low compile time and much simpler authoring experience. [Flash-attention 4](https://github.com/Dao-AILab/flash-attention/blob/672381f72c927a4b4a92f30755dc5829c3d0eaa3/flash_attn/cute/flash_fwd_sm100.py) was implemented in CuteDSL.

## Results
The provided cute dsl template by the contest has a latency of `~100µs`. My best CuteDSL kernel was `24.260μs`. The number 1 kernel was `18.550μs`. The results are in this [leaderboard](https://www.gpumode.com/v2/leaderboard/595?tab=rankings), which also has the submitted scripts.

# Simplify Compute
## Reduce SF compute
The template kernel loads and computes all the scaling factors.
```py
for i in cutlass.range_constexpr(mma_tiler_mnk[2]):
    res += tArA[i] * tArSFA[i] * tBrB[i] * tBrSFB[i]
```

We don't need to actually multiply for every a * b, only need to apply the scaling factors for every block of 16 values (`sf_vec_size` == 16).

```py
for i in cutlass.range_constexpr(mma_tiler_mnk[2] // sf_vec_size):
    acc_sf_block = cute.zeros_like(tCgC, cutlass.Float32)
    offset = i * sf_vec_size
    for j in cutlass.range_constexpr(sf_vec_size):
        acc_sf_block += tArA[offset + j] * tBrB[offset + j]
    res += acc_sf_block * tCrSF[i]
```

## fp8 -> fp16
FP4 are converted to FP16 in the template code. Under the hood it calls [cvt_f4e2m1_f16_intrinsic](https://docs.nvidia.com/cutlass/media/docs/pythonDSL/cute_dsl_api/cute_arch.html#cutlass.cute.arch.cvt_f4e2m1_f16_intrinsic). Simon shared in his [blog](https://veitner.bearblog.dev/demystifying-numeric-conversions-in-cutedsl/) that we could convert fp8 to fp16 instead of fp32 to reduce the latency by implementing a similar intrinsic. See implementation details in his blog.

## f16x2 ops
Also shared by Simon, we could use fp16x2 ops to speed up the fma.

```py
@dsl_user_op
def fma_f16x2(
    a: Tuple[Float16, Float16],
    b: Tuple[Float16, Float16],
    c: Tuple[Float16, Float16],
    *,
    loc=None,
    ip=None,
) -> Tuple[Float16, Float16]:
    # Pack two Float16 values into vector<2xf16>
    vec_type = ir.VectorType.get([2], Float16.mlir_type, loc=loc)

    vec_a = vector.from_elements(
        vec_type,
        [a[0].ir_value(loc=loc, ip=ip), a[1].ir_value(loc=loc, ip=ip)],
        loc=loc,
        ip=ip,
    )
    vec_b = vector.from_elements(
        vec_type,
        [b[0].ir_value(loc=loc, ip=ip), b[1].ir_value(loc=loc, ip=ip)],
        loc=loc,
        ip=ip,
    )
    vec_c = vector.from_elements(
        vec_type,
        [c[0].ir_value(loc=loc, ip=ip), c[1].ir_value(loc=loc, ip=ip)],
        loc=loc,
        ip=ip,
    )

    # Bitcast to i32 for PTX (f16x2 is packed into 32 bits)
    a_i32 = llvm.bitcast(Int32.mlir_type, vec_a, loc=loc, ip=ip)
    b_i32 = llvm.bitcast(Int32.mlir_type, vec_b, loc=loc, ip=ip)
    c_i32 = llvm.bitcast(Int32.mlir_type, vec_c, loc=loc, ip=ip)

    # Simple single-line PTX like cvt_f16x2_f32
    result_i32 = llvm.inline_asm(
        Int32.mlir_type,
        [a_i32, b_i32, c_i32],
        "fma.rn.f16x2 $0, $1, $2, $3;",
        "=r,r,r,r",
        has_side_effects=False,
        is_align_stack=False,
        asm_dialect=llvm.AsmDialect.AD_ATT,
        loc=loc,
        ip=ip,
    )

    # Bitcast back to vector<2xf16>
    vec_result = llvm.bitcast(vec_type, result_i32, loc=loc, ip=ip)

    # Extract results
    result0 = Float16(
        vector.extract(
            vec_result, dynamic_position=[], static_position=[0], loc=loc, ip=ip
        )
    )
    result1 = Float16(
        vector.extract(
            vec_result, dynamic_position=[], static_position=[1], loc=loc, ip=ip
        )
    )

    return result0, result1

# Usage
for i in cutlass.range_constexpr(k_tile_size // sf_vec_size // 2):
    sfBlock = cute.make_rmem_tensor((4,), c_dtype)
    sfBlock.fill(0.0)
    offset_1 = i * 2 * sf_vec_size
    offset_2 = (i * 2 + 1) * sf_vec_size
    for j in cutlass.range_constexpr(sf_vec_size // 2):
        j2 = j * 2
        offset_ele_1 = offset_1 + j2
        sfBlock[0], sfBlock[1] = fma_f16x2(
            (tArA[offset_ele_1], tArA[offset_ele_1 + 1]),
            (tBrB[offset_ele_1], tBrB[offset_ele_1 + 1]),
            (sfBlock[0], sfBlock[1]),
        )
        offset_ele_2 = offset_2 + j2
        sfBlock[2], sfBlock[3] = fma_f16x2(
            (tArA[offset_ele_2], tArA[offset_ele_2 + 1]),
            (tBrB[offset_ele_2], tBrB[offset_ele_2 + 1]),
            (sfBlock[2], sfBlock[3]),
        )

    sf1 = tCrSFA[offset_1] * tCrSFB[offset_1]
    sf2 = tCrSFA[offset_2] * tCrSFB[offset_2]
    res += (
        sfBlock[0] * sf1
        + sfBlock[1] * sf1
        + sfBlock[2] * sf2
        + sfBlock[3] * sf2
    )
```

## Combine everything
Cutlass lib writes a chunk of PTX code [link](https://github.com/NVIDIA/cutlass/blob/ec8daf642d69fc31352ac6fa6e14a0de9019604b/include/cutlass/gemm/kernel/gemv_blockscaled.h#L580) to do everything above. The only thing that's not mentioned above is using more registers and doing tree reduction to get block sum result. I implemented a cuteDSL equivalent tree reduction but didn't see boost on top of the current impl, probably because the CuteDSL compiler already generates the optimized code. I inspected the SASS code sees the registers don't have strict dependency.

Here's the function from the top 1 CuteDSL kernel by @lambda.

```py
@dsl_user_op
def blockscaled_multiply_add(
    frg_a_packed, frg_b_packed, frg_sfa_packed, frg_sfb_packed, *, loc=None, ip=None
) -> cute.Float16:
    out = llvm.inline_asm(
        cute.Float16.mlir_type,
        [
            frg_sfa_packed[0].ir_value(loc=loc, ip=ip),
            frg_sfb_packed[0].ir_value(loc=loc, ip=ip),
            frg_a_packed[0].ir_value(loc=loc, ip=ip),
            frg_b_packed[0].ir_value(loc=loc, ip=ip),
            frg_a_packed[1].ir_value(loc=loc, ip=ip),
            frg_b_packed[1].ir_value(loc=loc, ip=ip),
            frg_a_packed[2].ir_value(loc=loc, ip=ip),
            frg_b_packed[2].ir_value(loc=loc, ip=ip),
            frg_a_packed[3].ir_value(loc=loc, ip=ip),
            frg_b_packed[3].ir_value(loc=loc, ip=ip),
        ],
        # [gmem_ptr_i64, Float32(a).ir_value(loc=loc, ip=ip), cache_hint.ir_value()],
        "{\n"
        # declare registers for A / B tensors
        ".reg .b8 byte0_0, byte0_1, byte0_2, byte0_3;\n"
        ".reg .b8 byte0_4, byte0_5, byte0_6, byte0_7;\n"
        ".reg .b8 byte1_0, byte1_1, byte1_2, byte1_3;\n"
        ".reg .b8 byte1_4, byte1_5, byte1_6, byte1_7;\n"
        ".reg .b8 byte2_0, byte2_1, byte2_2, byte2_3;\n"
        ".reg .b8 byte2_4, byte2_5, byte2_6, byte2_7;\n"
        ".reg .b8 byte3_0, byte3_1, byte3_2, byte3_3;\n"
        ".reg .b8 byte3_4, byte3_5, byte3_6, byte3_7;\n"
        # declare registers for accumulators
        ".reg .f16x2 accum_0_0, accum_0_1, accum_0_2, accum_0_3;\n"
        ".reg .f16x2 accum_1_0, accum_1_1, accum_1_2, accum_1_3;\n"
        ".reg .f16x2 accum_2_0, accum_2_1, accum_2_2, accum_2_3;\n"
        ".reg .f16x2 accum_3_0, accum_3_1, accum_3_2, accum_3_3;\n"
        # declare registers for scaling factors
        ".reg .f16x2 sfa_f16x2;\n" ".reg .f16x2 sfb_f16x2;\n" ".reg .f16x2 sf_f16x2;\n"
        # declare registers for conversion
        ".reg .f16x2 cvt_0_0, cvt_0_1, cvt_0_2, cvt_0_3;\n"
        ".reg .f16x2 cvt_0_4, cvt_0_5, cvt_0_6, cvt_0_7;\n"
        ".reg .f16x2 cvt_1_0, cvt_1_1, cvt_1_2, cvt_1_3;\n"
        ".reg .f16x2 cvt_1_4, cvt_1_5, cvt_1_6, cvt_1_7;\n"
        ".reg .f16x2 cvt_2_0, cvt_2_1, cvt_2_2, cvt_2_3;\n"
        ".reg .f16x2 cvt_2_4, cvt_2_5, cvt_2_6, cvt_2_7;\n"
        ".reg .f16x2 cvt_3_0, cvt_3_1, cvt_3_2, cvt_3_3;\n"
        ".reg .f16x2 cvt_3_4, cvt_3_5, cvt_3_6, cvt_3_7;\n"
        ".reg .f16 result_f16, lane0, lane1;\n"
        ".reg .f16x2 mul_f16x2_0, mul_f16x2_1;\n"
        # convert scaling factors from fp8 to f16x2
        "cvt.rn.f16x2.e4m3x2 sfa_f16x2, $1;\n" "cvt.rn.f16x2.e4m3x2 sfb_f16x2, $2;\n"
        # clear accumulators
        "mov.b32 accum_0_0, 0;\n"
        "mov.b32 accum_0_1, 0;\n"
        "mov.b32 accum_0_2, 0;\n"
        "mov.b32 accum_0_3, 0;\n"
        "mov.b32 accum_1_0, 0;\n"
        "mov.b32 accum_1_1, 0;\n"
        "mov.b32 accum_1_2, 0;\n"
        "mov.b32 accum_1_3, 0;\n"
        "mov.b32 accum_2_0, 0;\n"
        "mov.b32 accum_2_1, 0;\n"
        "mov.b32 accum_2_2, 0;\n"
        "mov.b32 accum_2_3, 0;\n"
        "mov.b32 accum_3_0, 0;\n"
        "mov.b32 accum_3_1, 0;\n"
        "mov.b32 accum_3_2, 0;\n"
        "mov.b32 accum_3_3, 0;\n"
        # multiply, unpacking and permuting scale factors
        "mul.rn.f16x2 sf_f16x2, sfa_f16x2, sfb_f16x2;\n"
        "mov.b32 {lane0, lane1}, sf_f16x2;\n"
        "mov.b32 mul_f16x2_0, {lane0, lane0};\n"
        "mov.b32 mul_f16x2_1, {lane1, lane1};\n"
        # unpacking A and B tensors
        "mov.b32 {byte0_0, byte0_1, byte0_2, byte0_3}, $3;\n"
        "mov.b32 {byte0_4, byte0_5, byte0_6, byte0_7}, $4;\n"
        "mov.b32 {byte1_0, byte1_1, byte1_2, byte1_3}, $5;\n"
        "mov.b32 {byte1_4, byte1_5, byte1_6, byte1_7}, $6;\n"
        "mov.b32 {byte2_0, byte2_1, byte2_2, byte2_3}, $7;\n"
        "mov.b32 {byte2_4, byte2_5, byte2_6, byte2_7}, $8;\n"
        "mov.b32 {byte3_0, byte3_1, byte3_2, byte3_3}, $9;\n"
        "mov.b32 {byte3_4, byte3_5, byte3_6, byte3_7}, $10;\n"
        # convert A and B tensors from fp4 to f16x2
        # A[0 - 7] and B[0 - 7]
        "cvt.rn.f16x2.e2m1x2 cvt_0_0, byte0_0;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_1, byte0_1;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_2, byte0_2;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_3, byte0_3;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_4, byte0_4;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_5, byte0_5;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_6, byte0_6;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_7, byte0_7;\n"
        # A[8 - 15] and B[8 - 15]
        "cvt.rn.f16x2.e2m1x2 cvt_1_0, byte1_0;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_1, byte1_1;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_2, byte1_2;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_3, byte1_3;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_4, byte1_4;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_5, byte1_5;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_6, byte1_6;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_7, byte1_7;\n"
        # A[16 - 23] and B[16 - 23]
        "cvt.rn.f16x2.e2m1x2 cvt_2_0, byte2_0;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_1, byte2_1;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_2, byte2_2;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_3, byte2_3;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_4, byte2_4;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_5, byte2_5;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_6, byte2_6;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_7, byte2_7;\n"
        # A[24 - 31] and B[24 - 31]
        "cvt.rn.f16x2.e2m1x2 cvt_3_0, byte3_0;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_1, byte3_1;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_2, byte3_2;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_3, byte3_3;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_4, byte3_4;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_5, byte3_5;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_6, byte3_6;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_7, byte3_7;\n"
        # fma for A[0 - 7] and B[0 - 7]
        "fma.rn.f16x2 accum_0_0, cvt_0_0, cvt_0_4, accum_0_0;\n"
        "fma.rn.f16x2 accum_0_1, cvt_0_1, cvt_0_5, accum_0_1;\n"
        "fma.rn.f16x2 accum_0_2, cvt_0_2, cvt_0_6, accum_0_2;\n"
        "fma.rn.f16x2 accum_0_3, cvt_0_3, cvt_0_7, accum_0_3;\n"
        # fma for A[8 - 15] and B[8 - 15]
        "fma.rn.f16x2 accum_1_0, cvt_1_0, cvt_1_4, accum_1_0;\n"
        "fma.rn.f16x2 accum_1_1, cvt_1_1, cvt_1_5, accum_1_1;\n"
        "fma.rn.f16x2 accum_1_2, cvt_1_2, cvt_1_6, accum_1_2;\n"
        "fma.rn.f16x2 accum_1_3, cvt_1_3, cvt_1_7, accum_1_3;\n"
        # fma for A[16 - 23] and B[16 - 23]
        "fma.rn.f16x2 accum_2_0, cvt_2_0, cvt_2_4, accum_2_0;\n"
        "fma.rn.f16x2 accum_2_1, cvt_2_1, cvt_2_5, accum_2_1;\n"
        "fma.rn.f16x2 accum_2_2, cvt_2_2, cvt_2_6, accum_2_2;\n"
        "fma.rn.f16x2 accum_2_3, cvt_2_3, cvt_2_7, accum_2_3;\n"
        # fma for A[24 - 31] and B[24 - 31]
        "fma.rn.f16x2 accum_3_0, cvt_3_0, cvt_3_4, accum_3_0;\n"
        "fma.rn.f16x2 accum_3_1, cvt_3_1, cvt_3_5, accum_3_1;\n"
        "fma.rn.f16x2 accum_3_2, cvt_3_2, cvt_3_6, accum_3_2;\n"
        "fma.rn.f16x2 accum_3_3, cvt_3_3, cvt_3_7, accum_3_3;\n"
        # tree reduction for accumulators
        "add.rn.f16x2 accum_0_0, accum_0_0, accum_0_1;\n"
        "add.rn.f16x2 accum_0_2, accum_0_2, accum_0_3;\n"
        "add.rn.f16x2 accum_1_0, accum_1_0, accum_1_1;\n"
        "add.rn.f16x2 accum_1_2, accum_1_2, accum_1_3;\n"
        "add.rn.f16x2 accum_2_0, accum_2_0, accum_2_1;\n"
        "add.rn.f16x2 accum_2_2, accum_2_2, accum_2_3;\n"
        "add.rn.f16x2 accum_3_0, accum_3_0, accum_3_1;\n"
        "add.rn.f16x2 accum_3_2, accum_3_2, accum_3_3;\n"
        "add.rn.f16x2 accum_0_0, accum_0_0, accum_0_2;\n"
        "add.rn.f16x2 accum_1_0, accum_1_0, accum_1_2;\n"
        "add.rn.f16x2 accum_2_0, accum_2_0, accum_2_2;\n"
        "add.rn.f16x2 accum_3_0, accum_3_0, accum_3_2;\n"
        "add.rn.f16x2 accum_0_0, accum_0_0, accum_1_0;\n"
        "add.rn.f16x2 accum_2_0, accum_2_0, accum_3_0;\n"
        # apply scaling factors and final reduction
        "mul.rn.f16x2 accum_0_0, mul_f16x2_0, accum_0_0;\n"
        "mul.rn.f16x2 accum_2_0, mul_f16x2_1, accum_2_0;\n"
        "add.rn.f16x2 accum_0_0, accum_0_0, accum_2_0;\n"
        "mov.b32 {lane0, lane1}, accum_0_0;\n"
        "add.rn.f16 result_f16, lane0, lane1;\n"
        "mov.b16 $0, result_f16;\n"
        "}\n",
        "=h, h, h, r, r, r, r, r, r, r, r",
        has_side_effects=False,
        is_align_stack=False,
        asm_dialect=llvm.AsmDialect.AD_ATT,
    )

    return out
```

# SM Utilization
## Increase the number of blocks
The template uses

## Increase the number of threads in each block
Increasing the number of blocks naively will reduce the number of threads in each block. Since each SM has 128 cuda cores, we need at least 128 threads per thread block to parallelize the compute.

A way to increase the block size is using more threads to process each row. The intermediate values of every row could spread across different warps, so they need to write to shared mem.


## Reuduce Smem Access
To reduce smem usage, we could transpose the thread block so that continuous tidx process a row, and different tidy processes different rows. With this we could avoid smem read/write and get a row result through warp reduce.

Borrowed from [here](https://github.com/Dao-AILab/flash-attention/blob/672381f72c927a4b4a92f30755dc5829c3d0eaa3/flash_attn/cute/utils.py#L136)
```py
@cute.jit
def warp_reduce_sum(
    val: cute.TensorSSA | cute.Numeric,
    width: cutlass.Constexpr[int] = cute.arch.WARP_SIZE,
) -> cute.TensorSSA | cute.Numeric:
    if cutlass.const_expr(isinstance(val, cute.TensorSSA)):
        res = cute.make_fragment(val.shape, val.dtype)
        res.store(val)
        for i in cutlass.range_constexpr(cute.size(val.shape)):
            res[i] = warp_reduce_sum(res[i], width)
        return res.load()
    else:
        for i in cutlass.range_constexpr(int(math.log2(width))):
            val += cute.arch.shuffle_sync_bfly(val, offset=1 << i)
    return val

# usage
out = warp_reduce_sum(res, width=num_col_threads)
```

# Auto Tune
different tile sizes

# CuteDSL Basics
Tensor
TensorSSA
Numeric


# Things didn't help

Other than the successful techniques, I also tried a bunch of other optimizations. Most of ops / optimizations that involve shared memory (smem) didn't help the performance, mostly due to the small problem sizes.

# cp async
CP async was introduced in A100 which allows overlapping compute and data loading. The restriction is that it can only load to shared mem, and max data size for each instruction is 128 bits.

```py
```

multi buffer
```py
```

Wasn't able to make the prefecth stages work with the the official doc.

# Swizzle in smem
NCU analysis shows bank conflict in shared mem access and an estimated speedup of 80%.

Implemented swizzle
```py
```

## Smem Tree Reduction
Originally it compute the some of each row iteratively by reading every entry of the smem of the row.
```py
if tidy == 0:
    out = cute.zeros_like(tCgC, acc_dtype)
    for i in cutlass.range_constexpr(threads_per_k):
        out += shared_res[(tidx, i)]

    tCgC.store(out.to(c_dtype))
```

Update to do tree reduction where multiple threads access the shared mem and compute the some. This reduces the number of adds op from O(n) to O(logn)

```py
x = threads_per_k
while tidy < (x // 2):
    shared_res[(tidx, tidy)] += shared_res[(tidx, tidy + (x // 2))]
    cute.arch.sync_threads()
    x = x // 2
out = cute.zeros_like(tCgC, acc_dtype)
out += shared_res[(tidx, 0)]
```
However it didn't help because of
1. More writes to the shared mem, which is slower than writing to the register.
2. Multiple sync threads, which could create overheads.
3. The problem size is small - reduce from 16 to 4, which doesn't balance out the overheads it creates.

# Other's kernels
Directly use a large chunk of PTX code for multiply-add