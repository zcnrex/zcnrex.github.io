---
layout: post
title:  "Blackwell NVFP4 Kernel Hackathon Part 1: GEMV kernel"
date:   2025-12-01 15:18:18 +0000
tags: gemv,nvfp4,cutedsl
---

# Introduction
## The contest
There is a contest https://luma.com/9n27uem4?tk=EfycNU to develop the fastest kernels for [NVFP4](https://docs.nvidia.com/cutlass/media/docs/cpp/fundamental_types.html) format, a low precision micro-block scaling value format, on Blackwell B200. It involves 4 kernels: GEMV ([leaderboard])(https://www.gpumode.com/v2/leaderboard/595?tab=rankings), GEMM, Gated Dual GEMM, and Grouped GEMM.

This blogpost dives into optimization techniques of the first kernel GEMV that was conclueded on 11/29/2025. (It doesn't include the latency improvement numbers for optimizations shared because a different combination of the optimizations can have different effects.)

## NVFP4 format
It's a micro-block scaling low precision value format. Each value block contains 16 FP4 numbers (E2M1) plus 1 shared FP8 scale (E4M3). Other micro-block scaling formats include MXFP8 and MXFP4 that have different block size, value precisions, and scaling factors.

NVFP4 achieves a good trade-off between the hardware efficiency and model performance. Here's an Nvidia [blog](https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/) that dives deep into NVFP4 efficiency and compares different block scaled low precision data formats.

## Problem sizes
Here are the problem sizes of kernel 1 and the speed of light analysis of B200:

|M  |  K | L | time\[µs] |
| - | - | - | - |
| 7168 | 16384 | 1| 8.622 |
| 4096 | 7168 | 8 | 17.275 |
| 7168 | 2048 | 4 | 4.317 |

## CuteDSL Reference Template
[CuteDSL](https://docs.nvidia.com/cutlass/media/docs/pythonDSL/overview.html) is a python DSL for CUTLASS, a high performance CUDA kernel template library. It achieves great kernel performance with extra low compile time and much simpler authoring experience. [Flash-attention 4](https://github.com/Dao-AILab/flash-attention/blob/672381f72c927a4b4a92f30755dc5829c3d0eaa3/flash_attn/cute/flash_fwd_sm100.py) was implemented in CuteDSL. The contest provides [a CuteDSL template reference code](https://github.com/gpu-mode/reference-kernels/blob/5fbab1c58998991f0564584170283bba49e1bb42/problems/nvidia/nvfp4_gemv/template_cute.py) as a starting point.

## Results
The provided cute dsl template by the contest has a latency of `~100µs`. My best CuteDSL kernel was `24.260μs`. The number 1 kernel was `18.550μs` by `@s.am._`. The number 1 CuteDSL kernel was `21.598μs` by `@lambda`. The results are in this [leaderboard](https://www.gpumode.com/v2/leaderboard/595?tab=rankings), which also has submitted scripts by all participants.

# Simplify Compute
## Reduce SF loading
Originally the template kernel loads all repeated scaling factor numbers.
```py
tAgSFA = gSFA_mkl[tidy, None, bidx, k_tile, bidz]
tBgSFB = gSFB_nkl[0, None, bidy, k_tile, bidz]
```

[Yue](https://www.linkedin.com/in/yue-zhang-ishere/) pointed out that we only need to load 1 scaling factor per block.
```py
# SF shape: [block_M, block_K, rest_M, rest_K, rest_L]
# Where, block_M = (32, ?); block_K = (16, ?);
# The exact block_M and block_K shapes are determined by the tiler values.
tAgSFA = gSFA_mkl[tidy, (0, None), bidx, k_tile, bidz]
tBgSFB = gSFB_nkl[0, (0, None), bidy, k_tile, bidz]
```

## Reduce SF compute
The template kernel loads and computes all the scaling factors.
```py
for i in cutlass.range_constexpr(mma_tiler_mnk[2]):
    res += tArA[i] * tArSFA[i] * tBrB[i] * tBrSFB[i]
```

We don't need to actually multiply for every a * b, only need to apply the scaling factors for every block of 16 values (`sf_vec_size` == 16). This is the logic implemented in the cutlass [gemv_blockscaled.h](https://github.com/NVIDIA/cutlass/blob/ec8daf642d69fc31352ac6fa6e14a0de9019604b/include/cutlass/gemm/kernel/gemv_blockscaled.h#L770) kernel.

```py
for i in cutlass.range_constexpr(mma_tiler_mnk[2] // sf_vec_size):
    acc_sf_block = cute.zeros_like(tCgC, cutlass.Float32)
    offset = i * sf_vec_size
    for j in cutlass.range_constexpr(sf_vec_size):
        acc_sf_block += tArA[offset + j] * tBrB[offset + j]
    res += acc_sf_block * tCrSF[i]
```

## FP4 -> FP16
FP4 are converted to FP32 in the template code. CuteDSL natively support converting FP4 to FP16 using [cvt_f4e2m1_f16_intrinsic](https://docs.nvidia.com/cutlass/media/docs/pythonDSL/cute_dsl_api/cute_arch.html#cutlass.cute.arch.cvt_f4e2m1_f16_intrinsic). As shared by Yue, FP16 ops has higher compute throughput than FP32, so I converted all FP4 to FP16 instead.

## FP8 -> FP16
Simon shared in his [blog](https://veitner.bearblog.dev/demystifying-numeric-conversions-in-cutedsl/) that we could convert fp8 to fp16 instead of fp32 to reduce the latency by implementing a similar intrinsic. See implementation details in his blog.

## F16x2 ops
Also shared by Simon, we could use fp16x2 ops to speed up the fma.

```py
@dsl_user_op
def fma_f16x2(
    a: Tuple[Float16, Float16],
    b: Tuple[Float16, Float16],
    c: Tuple[Float16, Float16],
    *,
    loc=None,
    ip=None,
) -> Tuple[Float16, Float16]:
    # Pack two Float16 values into vector<2xf16>
    vec_type = ir.VectorType.get([2], Float16.mlir_type, loc=loc)

    vec_a = vector.from_elements(
        vec_type,
        [a[0].ir_value(loc=loc, ip=ip), a[1].ir_value(loc=loc, ip=ip)],
        loc=loc,
        ip=ip,
    )
    vec_b = vector.from_elements(
        vec_type,
        [b[0].ir_value(loc=loc, ip=ip), b[1].ir_value(loc=loc, ip=ip)],
        loc=loc,
        ip=ip,
    )
    vec_c = vector.from_elements(
        vec_type,
        [c[0].ir_value(loc=loc, ip=ip), c[1].ir_value(loc=loc, ip=ip)],
        loc=loc,
        ip=ip,
    )

    # Bitcast to i32 for PTX (f16x2 is packed into 32 bits)
    a_i32 = llvm.bitcast(Int32.mlir_type, vec_a, loc=loc, ip=ip)
    b_i32 = llvm.bitcast(Int32.mlir_type, vec_b, loc=loc, ip=ip)
    c_i32 = llvm.bitcast(Int32.mlir_type, vec_c, loc=loc, ip=ip)

    # Simple single-line PTX like cvt_f16x2_f32
    result_i32 = llvm.inline_asm(
        Int32.mlir_type,
        [a_i32, b_i32, c_i32],
        "fma.rn.f16x2 $0, $1, $2, $3;",
        "=r,r,r,r",
        has_side_effects=False,
        is_align_stack=False,
        asm_dialect=llvm.AsmDialect.AD_ATT,
        loc=loc,
        ip=ip,
    )

    # Bitcast back to vector<2xf16>
    vec_result = llvm.bitcast(vec_type, result_i32, loc=loc, ip=ip)

    # Extract results
    result0 = Float16(
        vector.extract(
            vec_result, dynamic_position=[], static_position=[0], loc=loc, ip=ip
        )
    )
    result1 = Float16(
        vector.extract(
            vec_result, dynamic_position=[], static_position=[1], loc=loc, ip=ip
        )
    )

    return result0, result1

# Usage
for i in cutlass.range_constexpr(k_tile_size // sf_vec_size // 2):
    sfBlock = cute.make_rmem_tensor((4,), c_dtype)
    sfBlock.fill(0.0)
    offset_1 = i * 2 * sf_vec_size
    offset_2 = (i * 2 + 1) * sf_vec_size
    for j in cutlass.range_constexpr(sf_vec_size // 2):
        j2 = j * 2
        offset_ele_1 = offset_1 + j2
        sfBlock[0], sfBlock[1] = fma_f16x2(
            (tArA[offset_ele_1], tArA[offset_ele_1 + 1]),
            (tBrB[offset_ele_1], tBrB[offset_ele_1 + 1]),
            (sfBlock[0], sfBlock[1]),
        )
        offset_ele_2 = offset_2 + j2
        sfBlock[2], sfBlock[3] = fma_f16x2(
            (tArA[offset_ele_2], tArA[offset_ele_2 + 1]),
            (tBrB[offset_ele_2], tBrB[offset_ele_2 + 1]),
            (sfBlock[2], sfBlock[3]),
        )

    sf1 = tCrSFA[offset_1] * tCrSFB[offset_1]
    sf2 = tCrSFA[offset_2] * tCrSFB[offset_2]
    res += (
        sfBlock[0] * sf1
        + sfBlock[1] * sf1
        + sfBlock[2] * sf2
        + sfBlock[3] * sf2
    )
```

## Combine everything
Cutlass lib writes a chunk of PTX code in [gemv_blockscaled.h](https://github.com/NVIDIA/cutlass/blob/ec8daf642d69fc31352ac6fa6e14a0de9019604b/include/cutlass/gemm/kernel/gemv_blockscaled.h#L580) to do everything above. The Cutlass lib has 1 additional optimization doing tree reduction to get the block sum result by using more registers. I implemented a cuteDSL equivalent tree reduction but didn't see boost on top of the current impl, probably because the CuteDSL compiler already generates the optimized code. I inspected the SASS code sees the registers don't have strict dependency.

The top 1 CuteDSL kernel by @lambda directly integrates the PTX code chunk. See the bottom section.

# SM Utilization
## Increase the number of blocks
For problem 1, the template uses 56 blocks, significantly under utilize B200 which has 148 SMs. The template grid size is `grid = (cute.ceil_div(c_tensor.shape[0], 128), 1, c_tensor.shape[2],)`. It's divided by 128 because each m tile is 128 so each block needs 128 threads each processes one row. We can increase the number of blocks simply by reducing the m tile to a smaller number like 64 or 32.

## Increase the number of threads in each block
Increasing the number of blocks naively will reduce the number of threads in each block. Since each SM has 128 cuda cores, we need at least 128 threads per thread block to parallelize the compute.

A way to increase the block size is using more threads to process each row. The intermediate values of every row could spread across different warps, so they need to write to shared mem. This again is an idea shared in Simon's [blog](https://veitner.bearblog.dev/nvfp4-gemv-improved/)

```py
def kernel(...):
    ...
    allocator = cutlass.utils.SmemAllocator()
    smem_layout = cute.make_layout(
        (num_row_threads, num_col_threads), stride=(num_col_threads, 1)
    )
    shared_res = allocator.allocate_tensor(
        element_type=acc_dtype, layout=smem_layout
    )

    for k_tile in range(tidy, k_tile_cnt, threads_per_k, unroll_full=True):
        ...

    shared_res[(tidx, tidy)] = res[0]
    cute.arch.sync_threads()

    if tidy == 0:
        out = cute.zeros_like(tCgC, acc_dtype)
        for i in cutlass.range_constexpr(num_col_threads):
            out += shared_res[(tidx, i)]

        tCgC.store(out.to(c_dtype))

# Kernel launch block config
block=[num_row_threads, num_col_threads, 1],
```

## Reuduce Smem Access
To reduce smem usage, we could transpose the thread block specified above so that continuous tidx process a row, and different tidy processes different rows. With this we could avoid smem read/write and get a row result through warp reduce.

Borrowed from [here](https://github.com/Dao-AILab/flash-attention/blob/672381f72c927a4b4a92f30755dc5829c3d0eaa3/flash_attn/cute/utils.py#L136)
```py
@cute.jit
def warp_reduce_sum(
    val: cute.TensorSSA | cute.Numeric,
    width: cutlass.Constexpr[int] = cute.arch.WARP_SIZE,
) -> cute.TensorSSA | cute.Numeric:
    if cutlass.const_expr(isinstance(val, cute.TensorSSA)):
        res = cute.make_fragment(val.shape, val.dtype)
        res.store(val)
        for i in cutlass.range_constexpr(cute.size(val.shape)):
            res[i] = warp_reduce_sum(res[i], width)
        return res.load()
    else:
        for i in cutlass.range_constexpr(int(math.log2(width))):
            val += cute.arch.shuffle_sync_bfly(val, offset=1 << i)
    return val

# Usage
for k_tile in range(tidx, k_tile_cnt, num_col_threads, unroll_full=True):
    ...

# num_col_threads isn't restricted to 32, only needs to be power of 2.
out = warp_reduce_sum(res, width=num_col_threads)

# Kernel launch block config
block=[num_col_threads, num_row_threads, 1],
```

# Auto Tune
Starting from cutlass 4.3.0, CuteDSL supports auto_tuning with specified search space of different configs. Since only the 3 problem sizes are benchmarked, only tune those sizes, and only tune when needed. Auto tuning details are in the [CuteDSL tutorial notebook](https://github.com/NVIDIA/cutlass/blob/ec8daf642d69fc31352ac6fa6e14a0de9019604b/examples/python/CuTeDSL/notebooks/benchmark_autotune.ipynb).

```py
import cutlass.cute.testing as testing

gemv = Nvfp4BatchedGemvKernel()

def tune_func(
    a_ptr,
    b_ptr,
    sfa_ptr,
    sfb_ptr,
    c_ptr,
    problem_size,
    num_row_threads=4,
    num_col_threads=32,
    k_tile_size=128,
):
    compiled_func = cute.compile(
        gemv,
        a_ptr,
        b_ptr,
        sfa_ptr,
        sfb_ptr,
        c_ptr,
        problem_size,
        num_row_threads,
        num_col_threads,
        k_tile_size,
    )
    return lambda: compiled_func(
        a_ptr,
        b_ptr,
        sfa_ptr,
        sfb_ptr,
        c_ptr,
        problem_size,
    )

auto_tune = True
if problem_size == (7168, 1, 16384, 1):
    if auto_tune:
        params = testing.tune(
            tune_func,
            params_dict={
                "num_row_threads": [8, 16, 32],
                "num_col_threads": [4, 8, 16],
                "k_tile_size": [32, 64, 128],
            },
            kernel_arguments=testing.JitArguments(
                a_ptr, b_ptr, sfa_ptr, sfb_ptr, c_ptr, problem_size
            ),
        )
    else:
        params = {
            "num_row_threads": 8,
            "num_col_threads": 32,
            "k_tile_size": 32,
        }
elif problem_size == (4096, 1, 7168, 8):
    ...
elif problem_size == (7168, 1, 2048, 4):
    ...
else:
    ...

print(f"The best kernel configs found: {params}")
_compiled_kernel_cache[problem_size] = cute.compile(
    gemv, a_ptr, b_ptr, sfa_ptr, sfb_ptr, c_ptr, problem_size, **params
)

# Update the kernel and __call__() signature with additional constexpr args
@cute.jit
def __call__(
    self,
    ...
    num_row_threads: cutlass.Constexpr = 4,
    num_col_threads: cutlass.Constexpr = 32,
    k_tile_size: cutlass.Constexpr = 128,
):
```

# Things didn't help

Other than the successful techniques above, I also tried a bunch of other optimizations. Most of ops / optimizations that involve shared memory (smem) didn't help the performance, mostly due to the small problem sizes.

## cp async
CP async was introduced in A100 which allows overlapping compute and data loading. The restriction is that it can only load to shared mem, and max data size for each instruction is 128 bits.

I wasn't able to make the prefecth stages work with the the official doc on [Software Pipelining](https://docs.nvidia.com/cutlass/media/docs/pythonDSL/cute_dsl_general/dsl_control_flow.html#software-pipelining). So I manually implemented the pipelining.

Here's relevant code of cp async (adapted from [quack/copy_utils.py](https://github.com/Dao-AILab/quack/blob/bceb632dbac9bb0b55d48a7ed3ad204bd952fcb2/quack/copy_utils.py)) with multi buffer support & software pipelining.

```py
@dsl_user_op
def copy(
    src: cute.Tensor,
    dst: cute.Tensor,
    *,
    pred: Optional[cute.Tensor] = None,
    num_copy_elems: int = 1,
    loc=None,
    ip=None,
    **kwargs,
) -> None:
    copy_atom = cute.make_copy_atom(cpasync.CopyG2SOp(), dtype, num_bits_per_copy=num_copy_bits)
    cute.copy(copy_atom, src, dst, pred=pred, loc=loc, ip=ip, **kwargs)

# Usage
def kernel():
    ...
    allocator = cutlass.utils.SmemAllocator()

    # Allocate smem with multiple buffers
    a_smem_layout = cute.make_layout(
        (self.num_buffers, num_row_threads, num_col_threads, k_tile_size),
        stride=(
            num_row_threads * num_col_threads * k_tile_size,
            num_col_threads * k_tile_size,
            k_tile_size,
            1,
        ),
    )

    sA = allocator.allocate_tensor(element_type=ab_dtype, layout=a_smem_layout)
    tAsA = sA[None, tidy, tidx, None]
    k_tile_cnt = gA_mkl.layout[3].shape

    # Prefetch
    for i in cutlass.range_constexpr(self.prefetch_stages):
        if tidx + i * num_col_threads < k_tile_cnt:
            tAgA = gA_mkl[tidy, None, bidx, tidx + i * num_col_threads, bidz]
            copy(tAgA, tAsA[(i, None)], num_copy_elems=32, is_async=True,)
            cute.arch.cp_async_commit_group()

    for k_tile in range(tidx, k_tile_cnt, num_col_threads, unroll_full=True,):
        ...
        # Main loop fetch
        if k_tile + self.prefetch_stages * num_col_threads < k_tile_cnt:
            tAgA = gA_mkl[tidy, None, bidx, k_tile + self.prefetch_stages * num_col_threads, bidz,]
            copy(
                tAgA,
                tAsA[
                    (
                        ((k_tile - tidx) // num_col_threads + self.prefetch_stages)
                        % self.num_buffers,
                        None,
                    )
                ],
                num_copy_elems=32,
                is_async=True,
            )
            cute.arch.cp_async_commit_group()

        buffer_idx = (k_tile - tidx) // num_col_threads % self.num_buffers

        # CuteDSL has restriction on the data type so having this if-else block
        if buffer_idx == 0:
            cute.arch.cp_async_wait_group(0)
        elif buffer_idx == 1:
            cute.arch.cp_async_wait_group(1)
        elif buffer_idx == 2:
            cute.arch.cp_async_wait_group(2)
        else:
            cute.arch.cp_async_wait_group(3)

        # Load data from shared mem
        a_val_nvfp4 = tAsA[(buffer_idx, None)].load()
        ...
```

## Swizzle in smem
NCU analysis shows bank conflict in shared mem access and an estimated speedup of 80%. I was able to remove the smem bank conflict with swizzling. However since the smem usage contributes very little to the total latency, the swizzle mechanism didn't improve the latency but increased a little bit.

Here's the code snippet with 2 different approaches:
```py
# Approach 1
smem_base_layout = cute.make_layout(
    shape=(threads_per_m, threads_per_k),
    stride=(threads_per_k, 1)
)
smem_layout = cute.make_composed_layout(
    cute.make_swizzle(3, 3, 3),
    0,
    smem_base_layout,
)
shared_res = allocator.allocate_tensor(
    element_type=acc_dtype, layout=smem_layout.outer, swizzle=smem_layout.inner
)

# Approach 2
SWIZZLE_STRIDE = 33
smem_base_layout = cute.make_layout(
    shape=(threads_per_m, threads_per_k),
    stride=(threads_per_k * SWIZZLE_STRIDE, SWIZZLE_STRIDE)
)

# Access the shared_res as usual but no more bank conflict
shared_res[(tidx, tidy)] = res[0]
```

## Smem Tree Reduction
Originally it compute the some of each row iteratively by reading every entry of the smem of the row.
```py
if tidy == 0:
    out = cute.zeros_like(tCgC, acc_dtype)
    for i in cutlass.range_constexpr(threads_per_k):
        out += shared_res[(tidx, i)]

    tCgC.store(out.to(c_dtype))
```

Update to do tree reduction where multiple threads access the shared mem and compute the some. This reduces the number of adds op from O(n) to O(logn).

```py
x = threads_per_k
while tidy < (x // 2):
    shared_res[(tidx, tidy)] += shared_res[(tidx, tidy + (x // 2))]
    cute.arch.sync_threads()
    x = x // 2
out = cute.zeros_like(tCgC, acc_dtype)
out += shared_res[(tidx, 0)]
```

However it didn't help because of
1. More writes to the shared mem, which is slower than writing to the register.
2. Multiple sync threads, which could create overheads.
3. The problem size is small - reduce from 16 to 4, which doesn't balance out the overheads it creates.

# Other's kernels
My kernel ranked ~20 among 208 participants. (The ranking is not exact because a few kernels before me use techniques that aren't allowed by the contest) Following optimizations helped achieve better results.

## PTX blockscaled_multiply_add()
Directly use a large chunk of PTX code for multiply-add from the top 1 CuteDSL kernel by @lambda:

```py
@dsl_user_op
def blockscaled_multiply_add(
    frg_a_packed, frg_b_packed, frg_sfa_packed, frg_sfb_packed, *, loc=None, ip=None
) -> cute.Float16:
    out = llvm.inline_asm(
        cute.Float16.mlir_type,
        [
            frg_sfa_packed[0].ir_value(loc=loc, ip=ip),
            frg_sfb_packed[0].ir_value(loc=loc, ip=ip),
            frg_a_packed[0].ir_value(loc=loc, ip=ip),
            frg_b_packed[0].ir_value(loc=loc, ip=ip),
            frg_a_packed[1].ir_value(loc=loc, ip=ip),
            frg_b_packed[1].ir_value(loc=loc, ip=ip),
            frg_a_packed[2].ir_value(loc=loc, ip=ip),
            frg_b_packed[2].ir_value(loc=loc, ip=ip),
            frg_a_packed[3].ir_value(loc=loc, ip=ip),
            frg_b_packed[3].ir_value(loc=loc, ip=ip),
        ],
        # [gmem_ptr_i64, Float32(a).ir_value(loc=loc, ip=ip), cache_hint.ir_value()],
        "{\n"
        # declare registers for A / B tensors
        ".reg .b8 byte0_0, byte0_1, byte0_2, byte0_3;\n"
        ".reg .b8 byte0_4, byte0_5, byte0_6, byte0_7;\n"
        ".reg .b8 byte1_0, byte1_1, byte1_2, byte1_3;\n"
        ".reg .b8 byte1_4, byte1_5, byte1_6, byte1_7;\n"
        ".reg .b8 byte2_0, byte2_1, byte2_2, byte2_3;\n"
        ".reg .b8 byte2_4, byte2_5, byte2_6, byte2_7;\n"
        ".reg .b8 byte3_0, byte3_1, byte3_2, byte3_3;\n"
        ".reg .b8 byte3_4, byte3_5, byte3_6, byte3_7;\n"
        # declare registers for accumulators
        ".reg .f16x2 accum_0_0, accum_0_1, accum_0_2, accum_0_3;\n"
        ".reg .f16x2 accum_1_0, accum_1_1, accum_1_2, accum_1_3;\n"
        ".reg .f16x2 accum_2_0, accum_2_1, accum_2_2, accum_2_3;\n"
        ".reg .f16x2 accum_3_0, accum_3_1, accum_3_2, accum_3_3;\n"
        # declare registers for scaling factors
        ".reg .f16x2 sfa_f16x2;\n" ".reg .f16x2 sfb_f16x2;\n" ".reg .f16x2 sf_f16x2;\n"
        # declare registers for conversion
        ".reg .f16x2 cvt_0_0, cvt_0_1, cvt_0_2, cvt_0_3;\n"
        ".reg .f16x2 cvt_0_4, cvt_0_5, cvt_0_6, cvt_0_7;\n"
        ".reg .f16x2 cvt_1_0, cvt_1_1, cvt_1_2, cvt_1_3;\n"
        ".reg .f16x2 cvt_1_4, cvt_1_5, cvt_1_6, cvt_1_7;\n"
        ".reg .f16x2 cvt_2_0, cvt_2_1, cvt_2_2, cvt_2_3;\n"
        ".reg .f16x2 cvt_2_4, cvt_2_5, cvt_2_6, cvt_2_7;\n"
        ".reg .f16x2 cvt_3_0, cvt_3_1, cvt_3_2, cvt_3_3;\n"
        ".reg .f16x2 cvt_3_4, cvt_3_5, cvt_3_6, cvt_3_7;\n"
        ".reg .f16 result_f16, lane0, lane1;\n"
        ".reg .f16x2 mul_f16x2_0, mul_f16x2_1;\n"
        # convert scaling factors from fp8 to f16x2
        "cvt.rn.f16x2.e4m3x2 sfa_f16x2, $1;\n" "cvt.rn.f16x2.e4m3x2 sfb_f16x2, $2;\n"
        # clear accumulators
        "mov.b32 accum_0_0, 0;\n"
        "mov.b32 accum_0_1, 0;\n"
        "mov.b32 accum_0_2, 0;\n"
        "mov.b32 accum_0_3, 0;\n"
        "mov.b32 accum_1_0, 0;\n"
        "mov.b32 accum_1_1, 0;\n"
        "mov.b32 accum_1_2, 0;\n"
        "mov.b32 accum_1_3, 0;\n"
        "mov.b32 accum_2_0, 0;\n"
        "mov.b32 accum_2_1, 0;\n"
        "mov.b32 accum_2_2, 0;\n"
        "mov.b32 accum_2_3, 0;\n"
        "mov.b32 accum_3_0, 0;\n"
        "mov.b32 accum_3_1, 0;\n"
        "mov.b32 accum_3_2, 0;\n"
        "mov.b32 accum_3_3, 0;\n"
        # multiply, unpacking and permuting scale factors
        "mul.rn.f16x2 sf_f16x2, sfa_f16x2, sfb_f16x2;\n"
        "mov.b32 {lane0, lane1}, sf_f16x2;\n"
        "mov.b32 mul_f16x2_0, {lane0, lane0};\n"
        "mov.b32 mul_f16x2_1, {lane1, lane1};\n"
        # unpacking A and B tensors
        "mov.b32 {byte0_0, byte0_1, byte0_2, byte0_3}, $3;\n"
        "mov.b32 {byte0_4, byte0_5, byte0_6, byte0_7}, $4;\n"
        "mov.b32 {byte1_0, byte1_1, byte1_2, byte1_3}, $5;\n"
        "mov.b32 {byte1_4, byte1_5, byte1_6, byte1_7}, $6;\n"
        "mov.b32 {byte2_0, byte2_1, byte2_2, byte2_3}, $7;\n"
        "mov.b32 {byte2_4, byte2_5, byte2_6, byte2_7}, $8;\n"
        "mov.b32 {byte3_0, byte3_1, byte3_2, byte3_3}, $9;\n"
        "mov.b32 {byte3_4, byte3_5, byte3_6, byte3_7}, $10;\n"
        # convert A and B tensors from fp4 to f16x2
        # A[0 - 7] and B[0 - 7]
        "cvt.rn.f16x2.e2m1x2 cvt_0_0, byte0_0;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_1, byte0_1;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_2, byte0_2;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_3, byte0_3;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_4, byte0_4;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_5, byte0_5;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_6, byte0_6;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_0_7, byte0_7;\n"
        # A[8 - 15] and B[8 - 15]
        "cvt.rn.f16x2.e2m1x2 cvt_1_0, byte1_0;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_1, byte1_1;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_2, byte1_2;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_3, byte1_3;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_4, byte1_4;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_5, byte1_5;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_6, byte1_6;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_1_7, byte1_7;\n"
        # A[16 - 23] and B[16 - 23]
        "cvt.rn.f16x2.e2m1x2 cvt_2_0, byte2_0;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_1, byte2_1;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_2, byte2_2;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_3, byte2_3;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_4, byte2_4;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_5, byte2_5;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_6, byte2_6;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_2_7, byte2_7;\n"
        # A[24 - 31] and B[24 - 31]
        "cvt.rn.f16x2.e2m1x2 cvt_3_0, byte3_0;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_1, byte3_1;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_2, byte3_2;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_3, byte3_3;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_4, byte3_4;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_5, byte3_5;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_6, byte3_6;\n"
        "cvt.rn.f16x2.e2m1x2 cvt_3_7, byte3_7;\n"
        # fma for A[0 - 7] and B[0 - 7]
        "fma.rn.f16x2 accum_0_0, cvt_0_0, cvt_0_4, accum_0_0;\n"
        "fma.rn.f16x2 accum_0_1, cvt_0_1, cvt_0_5, accum_0_1;\n"
        "fma.rn.f16x2 accum_0_2, cvt_0_2, cvt_0_6, accum_0_2;\n"
        "fma.rn.f16x2 accum_0_3, cvt_0_3, cvt_0_7, accum_0_3;\n"
        # fma for A[8 - 15] and B[8 - 15]
        "fma.rn.f16x2 accum_1_0, cvt_1_0, cvt_1_4, accum_1_0;\n"
        "fma.rn.f16x2 accum_1_1, cvt_1_1, cvt_1_5, accum_1_1;\n"
        "fma.rn.f16x2 accum_1_2, cvt_1_2, cvt_1_6, accum_1_2;\n"
        "fma.rn.f16x2 accum_1_3, cvt_1_3, cvt_1_7, accum_1_3;\n"
        # fma for A[16 - 23] and B[16 - 23]
        "fma.rn.f16x2 accum_2_0, cvt_2_0, cvt_2_4, accum_2_0;\n"
        "fma.rn.f16x2 accum_2_1, cvt_2_1, cvt_2_5, accum_2_1;\n"
        "fma.rn.f16x2 accum_2_2, cvt_2_2, cvt_2_6, accum_2_2;\n"
        "fma.rn.f16x2 accum_2_3, cvt_2_3, cvt_2_7, accum_2_3;\n"
        # fma for A[24 - 31] and B[24 - 31]
        "fma.rn.f16x2 accum_3_0, cvt_3_0, cvt_3_4, accum_3_0;\n"
        "fma.rn.f16x2 accum_3_1, cvt_3_1, cvt_3_5, accum_3_1;\n"
        "fma.rn.f16x2 accum_3_2, cvt_3_2, cvt_3_6, accum_3_2;\n"
        "fma.rn.f16x2 accum_3_3, cvt_3_3, cvt_3_7, accum_3_3;\n"
        # tree reduction for accumulators
        "add.rn.f16x2 accum_0_0, accum_0_0, accum_0_1;\n"
        "add.rn.f16x2 accum_0_2, accum_0_2, accum_0_3;\n"
        "add.rn.f16x2 accum_1_0, accum_1_0, accum_1_1;\n"
        "add.rn.f16x2 accum_1_2, accum_1_2, accum_1_3;\n"
        "add.rn.f16x2 accum_2_0, accum_2_0, accum_2_1;\n"
        "add.rn.f16x2 accum_2_2, accum_2_2, accum_2_3;\n"
        "add.rn.f16x2 accum_3_0, accum_3_0, accum_3_1;\n"
        "add.rn.f16x2 accum_3_2, accum_3_2, accum_3_3;\n"
        "add.rn.f16x2 accum_0_0, accum_0_0, accum_0_2;\n"
        "add.rn.f16x2 accum_1_0, accum_1_0, accum_1_2;\n"
        "add.rn.f16x2 accum_2_0, accum_2_0, accum_2_2;\n"
        "add.rn.f16x2 accum_3_0, accum_3_0, accum_3_2;\n"
        "add.rn.f16x2 accum_0_0, accum_0_0, accum_1_0;\n"
        "add.rn.f16x2 accum_2_0, accum_2_0, accum_3_0;\n"
        # apply scaling factors and final reduction
        "mul.rn.f16x2 accum_0_0, mul_f16x2_0, accum_0_0;\n"
        "mul.rn.f16x2 accum_2_0, mul_f16x2_1, accum_2_0;\n"
        "add.rn.f16x2 accum_0_0, accum_0_0, accum_2_0;\n"
        "mov.b32 {lane0, lane1}, accum_0_0;\n"
        "add.rn.f16 result_f16, lane0, lane1;\n"
        "mov.b16 $0, result_f16;\n"
        "}\n",
        "=h, h, h, r, r, r, r, r, r, r, r",
        has_side_effects=False,
        is_align_stack=False,
        asm_dialect=llvm.AsmDialect.AD_ATT,
    )

    return out

# Usage
coord_s = (None, i % total_stages)

frg_a_packed = cute.flat_divide(
    cute.recast_tensor(frg_a[coord_s], cute.Uint32), (4,)
)
frg_b_packed = cute.flat_divide(
    cute.recast_tensor(frg_b[coord_s], cute.Uint32), (4,)
)
frg_sfa_packed = cute.flat_divide(
    cute.recast_tensor(frg_sfa[coord_s], cute.Uint16), (1,)
)
frg_sfb_packed = cute.flat_divide(
    cute.recast_tensor(frg_sfb[coord_s], cute.Uint16), (1,)
)
out += blockscaled_multiply_add(
    frg_a_packed[(None, 0)],
    frg_b_packed[(None, 0)],
    frg_sfa_packed[(None, 0)],
    frg_sfb_packed[(None, 0)],
)
```

## TMA and tcgen05
The number 2 CuteDSL kernel by @sazc uses TMA for async data loading and tcgen05 for mma compute. Despite overheads created by using these powerful hardware units, the kernel achieved `21.779μs`, which is an impressive feat.

## More specialized tuning
The #1 kernel uses very specialized tuning, and many more blocks in some cases.
- k // 2 = 3584: grid(params.m / 4, 1, params.b)
- k // 2 = 8192: grid(params.m, 1, params.b)
- k // 2 = 1024: grid(params.m / 8, 1, params.b)

To have a very precise control, the kernel essentially implements everything in PTX and then wrap with CUDA code. Different problem sizes even use different load PTX, cache hint, f32x2 vs f16x2 etc.

```cpp
// Loading function with cache hint
__device__ __forceinline__ void load_block_32x2fp4(
    const __nv_fp4x2_e2m1* rowA,
    const __nv_fp4x2_e2m1* vecB,
    const uint16_t*        rowS_u16,
    const uint16_t*        vecS_u16,
    int                    elem_base,
    int                    block_base,
    uint64_t (&a_regs)[4],
    uint64_t (&b_regs)[4],
    uint16_t (&sfa_regs)[2],
    uint16_t (&sfb_regs)[2])
{
    uint64_t rowA_addr = reinterpret_cast<uint64_t>(rowA + elem_base);
    uint64_t vecB_addr = reinterpret_cast<uint64_t>(vecB + elem_base);

    asm volatile(
        "ld.global.L1::no_allocate.L2::evict_first.L2::256B.v4.u64 {%0, %1, %2, %3}, [%8];\n\t"
        "ld.global.L1::evict_last.L2::evict_last.v4.u64 {%4, %5, %6, %7}, [%9];\n\t"
        : "=l"(a_regs[0]), "=l"(a_regs[1]), "=l"(a_regs[2]), "=l"(a_regs[3]),
          "=l"(b_regs[0]), "=l"(b_regs[1]), "=l"(b_regs[2]), "=l"(b_regs[3])
        : "l"(rowA_addr), "l"(vecB_addr)
    );

    uint64_t rowS_addr = reinterpret_cast<uint64_t>(rowS_u16 + block_base * 2);
    uint64_t vecS_addr = reinterpret_cast<uint64_t>(vecS_u16 + block_base * 2);

    asm volatile(
        "ld.global.L1::no_allocate.v2.u16 {%0, %1}, [%4];\n\t"
        "ld.global.L1::evict_last.v2.u16 {%2, %3}, [%5];\n\t"
        : "=h"(sfa_regs[0]), "=h"(sfa_regs[1]),
          "=h"(sfb_regs[0]), "=h"(sfb_regs[1])
        : "l"(rowS_addr), "l"(vecS_addr)
    );
}
```