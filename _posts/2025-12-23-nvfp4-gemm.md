---
layout: post
title:  "Blackwell NVFP4 Kernel Hackathon Part 2: GEMM kernel"
date:   2025-12-23 09:18:18 +0000
tags: [gemm, nvfp4, cutedsl, B200]
---

## Contest Recap

[Blackwell NVFP4 Kernel Hackathon](https://luma.com/9n27uem4?tk=EfycNU) challenged developers to create high-performance kernels for the [NVFP4](https://docs.nvidia.com/cutlass/media/docs/cpp/fundamental_types.html) format. This format utilizes a low-precision micro-block scaling value specifically optimized for the Blackwell B200 architecture. The competition spanned four categories: GEMV ([leaderboard](https://www.gpumode.com/v2/leaderboard/595?tab=rankings)), GEMM ([leaderboard](https://www.gpumode.com/v2/leaderboard/597?tab=rankings)), Gated Dual GEMM, and Grouped GEMM.

This post details my implementation for the GEMM kernel, which secured 6th place out of over 100 participants. It focuses on the mechanics of scaling factors and architectural simplifications derived from the [NVIDIA example kernel](https://github.com/NVIDIA/cutlass/blob/main/examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py).

If you’re looking for a deep technical dive into `tcgen05.mma`, I highly recommend gau.nerst’s "GOATED" [blogpost](https://gau-nernst.github.io/tcgen05). Here, I’ll focus on the specific "hacks" and simplifications that pushed my kernel to the top of the pack.

## Problem Sizes

The following table outlines the problem sizes and their speed-of-light latency targets:

| M | N | K | L | latency\[µs] |
| - | - | - | - | - |
| 128 | 7168 | 16384 | 1| 8.994 |
| 128 | 4096 | 7168 | 1 | 2.354 |
| 128 | 7168 | 2048 | 1 | 1.333 |

## Understanding `tcgen05` for NVFP4

The block scaling GEMM formula is defined as: `(A * scale_A)  * (B * scale_B) + D`

<img src="{{site.baseurl}}/assets/nvfp4-gemm/blockscaled_gemm.png">

[Image source](https://developer.nvidia.com/blog/boosting-matrix-multiplication-speed-and-flexibility-with-nvidia-cublas-12-9/)

### Overall

The PTX section on `tcgen05` for block scaling formats: https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-block-scaling

<img src="{{site.baseurl}}/assets/nvfp4-gemm/tcgen05-mma-scale-vec-2x.png">

The diagram shows a visual of `tcgen05.mma` for block scaling formats. (It is for a different block scaling formats because it only has 2x; NVFP4 requres scale vector 4X.)

#### Data Flow

- A/B matrics: GMEM -> SMEM (optionally A can be copied to TMEM)
- SFA/SFB matrics: GMEM -> SMEM -> TMEM
- C/D matrics: TMEM -> register -> SMEM (optional) -> GMEM

#### Hardware Mapping / Instruction to Move Data

- GMEM -> SMEM: Handled by TMA
- SMEM -> TMEM: Executed via [tcgen05.cp](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-cp)
    - PTX: `tcgen05.cp.cta_group::1.32x128b.warpx4`
    - CuteDSL: `tcgen05.Cp4x32x128bOp(tcgen05.CtaGroup.ONE)`
- SMEM -> GMEM: Handled by TMA
- TMEM -> register: Executed via [tcgen05.ld](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-ld)
    - PTX: `tcgen05.ld.sync.aligned.32x32b.x128.b32`
    - CuteDSL: `tcgen05.Ld32x32bOp(tcgen05.Repetition.x128, tcgen05.Pack.NONE)`
- TMEM -> SMEM: not supported
- Register -> SMEM: `stmatrix`
    - CuteDSL: `cutlass.cute.nvgpu.warp.StMatrix16x8x8bOp`
- Register -> GMEM: sync copy
    - CuteDSL: `cute.nvgpu.CopyUniversalOp()`

#### NVFP4 PTX Gemm Instruction

```
tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.scale_vec::4X.collector_usage
                                [d-tmem],  a-desc,  b-desc, idesc,
                                [scale-A-tmem], [scale-B-tmem], enable-input-d;
```

- cta_group can be 1 or 2
- The other qualifiers are fixed for NVFP4
    - `.kind::mxf4nvf4`
    - `.scale_vec::4X` (or equivalent to `.block16`)

### Supported Tile Sizes

<img src="{{site.baseurl}}/assets/nvfp4-gemm/supported-tile-sizes.png">
[Source: PTX doc on `tcgen05` kind shapes](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-kind-shapes)

The table means, for NVFP4
- For 1 CTA group: it supports a shape of 128xNxK, where N = {8, 16, ...256} with steps of 8, and K = 64
- For 2 CTA groups: it supports a shape of 256xNxK, where N = {16, 32, ...256} with steps of 16, and K = 64

Additionally, although the table shows what sparse matrics shapes are supported, PTX doc mentions [here](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-sparse-matrices) that NVFP4 doesn't support sparse matrics, so K has to be 64. This restriction is implemented in [sm100_utils.make_blockscaled_trivial_tiled_mma()](https://github.com/NVIDIA/cutlass/blob/b7ecaa605dd70326900433695e11ebfec407edd2/python/CuTeDSL/cutlass/utils/blackwell_helpers.py#L952)

### SFB TMEM Layout

There aren't a lot of choices for M tile sizes, so the focus would be on tuning the N tile sizes. A primary hurdle in tuning N-tile sizes is understanding the Scale Factor B (SFB) layout in Tensor Memory (TMEM). `tcgen05` has a very specific layout requirement for NVFP4 SFB as documented here: https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-factor-b-layout-4x. However the documentation can be confusing because it illustrates only 32 lanes, while TMEM actually utilizes 128 lanes.

After hours of staring at diagrams, I realized the hardware "chops" the matrix every 32 rows and interleaves the sub-tiles.

<img src="{{site.baseurl}}/assets/nvfp4-gemm/b-vs-sfb-layout.png">

First, for a B matrix tile, it's shape N tile x K tile is 128 x (64 x 4). Using N tile 128 as an example which is the same as [num TMEM lanes](https://docs.nvidia.com/cuda/parallel-thread-execution/#tensor-memory-addressing). For an SBF tile, it's shape is 128 x (4 x 4) because NVFP4 block size is 16.

<img src="{{site.baseurl}}/assets/nvfp4-gemm/sfb-layout.png">

This diagram just zoom in on the indices: there are N0 to N127 rows, and SF0-SF3, SF4-SF7, SF8-SF11, SF12-SF15 columns for the 4 sub tiles.

<img src="{{site.baseurl}}/assets/nvfp4-gemm/sfb-tmem-layout.png">

The mapping logic involves partitioning the matrix into 32-row chunks; each chunk is assigned to a specific bank of 32 lanes, allowing the 128-lane TMEM to be addressed as four parallel 32-lane sectors. Expanding it to N == 255 will give us the same diagram as in https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-factor-b-layout-4x.

This explains some details in the [example kernel](https://github.com/NVIDIA/cutlass/blob/main/examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py):
- Why it requires 16 TMEM columns for SFB: because 128 / 32 * 4 = 16
- How does the [n tile = 64 offset](https://github.com/NVIDIA/cutlass/blob/b7ecaa605dd70326900433695e11ebfec407edd2/examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py#L1193) work: it needs to offset 64 / 32 * 4 = 8 columns. Using acc_ptr as the base ptr, since it's 32 bit vs 8 bit, it needs to offset 8 / (32 / 8) = 2 columns.

### The Example Kernel

The [base kernel](https://github.com/NVIDIA/cutlass/blob/main/examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py) provided by NVIDIA is highly optimized for large-scale production workloads, featuring advanced Blackwell warp specialization and persistent scheduling. The diagrams below illustrates the pipelining and state changes.

<img src="{{site.baseurl}}/assets/nvfp4-gemm/blackwell-ws.png">

In addition to the specialized data loading and gemm + epilogue warps in the H100, Blackwell architecture makes the epilogue stage separate specialized warps. This change is made possible by the TMEM hardware introduced by Blackwell.

<img src="{{site.baseurl}}/assets/nvfp4-gemm/blackwell-persistent-kernel.png">

This illustrates how different specialized warps interact with each other.
- TMA warp and MMA warp uses SMEM to signal, because TMA warp loads data from GMEM to SMEM. Then MMA warp either directly use data in SMEM for `tcgen05.mma` or copy to TMEM for the gemm
- MMA warp and Epilogue warps uses TMEM to signal each other. `tcgen05.mma` writes accumulation results to TMEM, then the epilogue warps read and process the accumulation results for the final compute.

## Simplifications & Optimizations

For the hackathon, I stripped the example kernel down and tuned it specifically for the provided problem sizes.

### 1. Tuning Tile Sizes for SM Occupancy

The n tile size from the template is 128. However, looking at our problem sizes, this leads to significant under-utilization of the B200:

* **Problem 1 & 3 ($N=7168$):** $7168 / 128 = 56$ SMs.
* **Problem 2 ($N=4096$):** $4096 / 128 = 32$ SMs.

By reducing the  tile size to $64$, we effectively double the number of active SMs:

* **Problem 1 & 3:** Increased to **$112$ SMs**.
* **Problem 2:** Increased to **$64$ SMs**.

```python
# SFB N tile size must remain at least 128 as tcgen05.ld requires it
self.mma_inst_shape_mn_sfb = (
    self.mma_inst_shape_mn[0] // (2 if self.use_2cta_instrs else 1),
    cute.round_up(self.mma_inst_shape_mn[1], 128),
)

# Find the correct GMEM slice for SFB as SFB N tile for reading is still 128
if cutlass.const_expr(self.cta_tile_shape_mnk[1] == 64):
    slice_n = mma_tile_coord_mnl[1] // 2
tBgSFB_slice = tBgSFB[
    (None, slice_n, None, mma_tile_coord_mnl[2])
]

# Offset TMEM Columns for N=64
if cutlass.const_expr(self.cta_tile_shape_mnk[1] == 64):
    # Shift by 2 columns (32-bit vs 8-bit recast logic)
    offset = cutlass.Int32((mma_tile_coord_mnl[1] % 2) * 2)
    shifted_ptr = cute.recast_ptr(
        acc_tmem_ptr + self.num_accumulator_tmem_cols + self.num_sfa_tmem_cols + offset,
        dtype=self.sf_dtype,
    )
```

### 2. Removing Persistent Scheduling

Persistent scheduling is excellent for load balancing across many "waves" of tiles with low overhead by removing block launches. However, our contest problems are small enough to be solved in a **single wave**. Removing the persistent scheduler eliminates the overhead of global atomic counters and `while` loops in the kernel prologue, as shown in the Example Kernel section, allowing us to map tiles directly using `bidx`, `bidy`, `bidz`.

### 3. Updating with Cutlass 4.3

Leveraging the latest `pipeline_init` abstractions from Cutlass 4.3 and enabling `defer_sync` improved the efficiency of cluster-level synchronization:

```python
# Update Cluster Initialization
# if cute.size(self.cluster_shape_mn) > 1:
#     cute.arch.cluster_arrive_relaxed()
pipeline_init_arrive(cluster_shape_mn=self.cluster_shape_mn, is_relaxed=True)

# if cute.size(self.cluster_shape_mn) > 1:
#     cute.arch.cluster_wait()
# else:
#     self.cta_sync_barrier.arrive_and_wait()
pipeline_init_wait(cluster_shape_mn=self.cluster_shape_mn)

# Added to pipeline creation for better overlap
defer_sync=True
```

### 4. Epilogue Streamlining

Since the workload fits in a single wave, certain synchronization barriers in the epilogue become redundant. Removing the unnecessary `fence_proxy` and `epilog_sync_barrier` reduced the latency between the completion of the MMA and the final TMA store to global memory.

```python
cute.arch.fence_proxy(
    cute.arch.ProxyKind.async_shared,
    space=cute.arch.SharedSpace.shared_cta,
)

# Removed redundant syncs for 1-wave compute
self.epilog_sync_barrier.arrive_and_wait()
```

## Further Optimizations

### TMA Multicast

Since the problem dimensions often involve multiple SMs sharing the same A matrix tiles, we can leverage the TMA Multicast capability. This hardware feature allows a single load from Global Memory (GMEM) to be delivered to the Shared Memory (SMEM) of multiple SMs within a cluster simultaneously. This reduces total GMEM bandwidth consumption and enforces data loading from the L2 cache, which is critical for bandwidth-bound scenarios.

### Prefetch Tiles

The current implementation operates with the TMA warp and MMA warp in tandem. A more advanced approach involves "prologue prefetching," where multiple TMA instructions are issued at the very beginning of the kernel without waiting for immediate MMA signals. This hides the initial global memory latency more effectively before entering the main loop, ensuring the MMA pipeline never starves for data.

### Split K

For GEMM problems where the $M$ and $N$ dimensions are small but $K$ is very large (as seen in Problem 1), the number of tiles can be too few to saturate all SMs. Split-K addresses this by partitioning the $K$ dimension into multiple slices that are computed in parallel by different SMs.

## Optimizations that Didn't Help Or Doesn't Work

### Directly store from register to gmem

The original implementation writes accumulation results to TMEM -> register -> SMEM -> GMEM. One would naturally wonder what happens if we remove the additional step of storing to SMEM, modify the data flow to be TMEM -> register -> GMEM. Thanks [Simon](https://www.linkedin.com/in/simon-veitner-174a681b6/) for helping with the implementation.

Unfortunately the implementation was slower. Will probably try a different way to copy - different tiling, CuteDSL functions, etc.

### Use N Tile Size 32

As calculated above, problem 2 only uses 64 SMs. It uses < 50% of B200 SMs, so natually we want to divide the matrices into smaller tiles. 1 simple way is using smaller N tile size. N tile size 32 should use 132 SMs. From a section above, [the PTX doc](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-kind-shapes) says N tile size 32 is supported.

However, the kernel throws error when I try to offset the SFB TMEM column by 4. Confirmed with Nvidia folks that there's restriction on the TMEM alignment. It's not in the official document though. A workaround is loading SFB from SMEM to register, then offset the registers when copying to TMEM. I didn't implement it due to the complexity and potentially worse performance with the additional copying to registers.

### Use only 1 Warp for Epilogue

C is stored in TMEM. 4 epilogue warps access the accumulate results, copy to SMEM then finally GMEM. 1 way to reduce the epilogue latency is removing sync needed across the warps after copying to SMEM. However this is not possible due to the [TMEM access restriction](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-tensor-memory-ld-st-access-restrictions): "each warp of a warpgroup in the CTA can access **a chunk** of the Tensor Memory". More specifically, warp 0 can access lay 0-31, it can't access lanes 32-127. That means if we want to reduce the num epilogue warps to 1, it requires changing the accumulated result layout, which isn't possible.

<img src="{{site.baseurl}}/assets/nvfp4-gemm/tmem-warp-mapping.png">

## Result

Ranking 6th among over 100 participants was a great milestone, especially having only focused on GEMM kernels for four months. Special thanks to [Simon](https://www.linkedin.com/in/simon-veitner-174a681b6/) and [Yue](https://www.linkedin.com/in/yue-zhang-ishere/) for the invaluable discussions and technical support.

## Resources
- https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-block-scaling
- https://veitner.bearblog.dev/blog/
- https://gau-nernst.github.io/tcgen05
- https://github.com/zartbot/blog/issues/3
- https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-1-introduction
- https://github.com/Dao-AILab/quack
- https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/cute/flash_fwd_sm100.py